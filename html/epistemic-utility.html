<!DOCTYPE html>
<!--[if lt IE 7]> <html class="ie6 ie"> <![endif]-->
<!--[if IE 7]>    <html class="ie7 ie"> <![endif]-->
<!--[if IE 8]>    <html class="ie8 ie"> <![endif]-->
<!--[if IE 9]>    <html class="ie9 ie"> <![endif]-->
<!--[if !IE]> --> <html lang="en"> <!-- <![endif]-->
<head>
<meta name="viewport" content="width=device-width, initial-scale=1.0" />
<title>
Epistemic Utility Arguments for Epistemic Norms (Stanford Encyclopedia of Philosophy)
</title>
<meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
<meta name="robots" content="noarchive, noodp" />
<meta property="citation_title" content="Epistemic Utility Arguments for Epistemic Norms" />
<meta property="citation_author" content="Pettigrew, Richard" />
<meta property="citation_publication_date" content="2011/09/23" />
<meta name="DC.title" content="Epistemic Utility Arguments for Epistemic Norms" />
<meta name="DC.creator" content="Pettigrew, Richard" />
<meta name="DCTERMS.issued" content="2011-09-23" />
<meta name="DCTERMS.modified" content="2023-10-11" />

<!-- NOTE: Import webfonts using this link: -->
<link href="https://fonts.googleapis.com/css?family=Source+Sans+Pro:400,300,600,200&amp;subset=latin,latin-ext" rel="stylesheet" type="text/css" />

<link rel="stylesheet" type="text/css" media="screen,handheld" href="../../css/bootstrap.min.css" />
<link rel="stylesheet" type="text/css" media="screen,handheld" href="../../css/bootstrap-responsive.min.css" />
<link rel="stylesheet" type="text/css" href="../../css/font-awesome.min.css" />
<!--[if IE 7]> <link rel="stylesheet" type="text/css" href="../../css/font-awesome-ie7.min.css"> <![endif]-->
<link rel="stylesheet" type="text/css" media="screen,handheld" href="../../css/style.css" />
<link rel="stylesheet" type="text/css" media="print" href="../../css/print.css" />
<link rel="stylesheet" type="text/css" href="../../css/entry.css" />
<!--[if IE]> <link rel="stylesheet" type="text/css" href="../../css/ie.css" /> <![endif]-->
<script type="text/javascript" src="../../js/jquery-1.9.1.min.js"></script>
<script type="text/javascript" src="../../js/bootstrap.min.js"></script>

<!-- NOTE: Javascript for sticky behavior needed on article and ToC pages -->
<script type="text/javascript" src="../../js/jquery-scrolltofixed-min.js"></script>
<script type="text/javascript" src="../../js/entry.js"></script>

<!-- SEP custom script -->
<script type="text/javascript" src="../../js/sep.js"></script>
</head>

<!-- NOTE: The nojs class is removed from the page if javascript is enabled. Otherwise, it drives the display when there is no javascript. -->
<body class="nojs article" id="pagetopright">
<div id="container">
<div id="header-wrapper">
  <div id="header">
    <div id="branding">
      <div id="site-logo"><a href="../../index.html"><img src="../../symbols/sep-man-red.png" alt="SEP home page" /></a></div>
      <div id="site-title"><a href="../../index.html">Stanford Encyclopedia of Philosophy</a></div>
    </div>
    <div id="navigation">
      <div class="navbar">
        <div class="navbar-inner">
          <div class="container">
            <button class="btn btn-navbar collapsed" data-target=".collapse-main-menu" data-toggle="collapse" type="button"> <i class="icon-reorder"></i> Menu </button>
            <div class="nav-collapse collapse-main-menu in collapse">
              <ul class="nav">
                <li class="dropdown open"><a id="drop1" href="#" class="dropdown-toggle" data-toggle="dropdown" role="button"><i class="icon-book"></i> Browse</a>
                  <ul class="dropdown-menu" role="menu" aria-labelledby="drop1">
                    <li role="menuitem"><a href="../../contents.html">Table of Contents</a></li>
                    <li role="menuitem"><a href="../../new.html">What's New</a></li>
                    <li role="menuitem"><a href="https://plato.stanford.edu/cgi-bin/encyclopedia/random">Random Entry</a></li>
                    <li role="menuitem"><a href="../../published.html">Chronological</a></li>
                    <li role="menuitem"><a href="../../archives/">Archives</a></li>
                  </ul>
                </li>
                <li class="dropdown open"><a id="drop2" href="#" class="dropdown-toggle" data-toggle="dropdown" role="button"><i class="icon-info-sign"></i> About</a>
                  <ul class="dropdown-menu" role="menu" aria-labelledby="drop2">
                    <li role="menuitem"><a href="../../info.html">Editorial Information</a></li>
                    <li role="menuitem"><a href="../../about.html">About the SEP</a></li>
                    <li role="menuitem"><a href="../../board.html">Editorial Board</a></li>
                    <li role="menuitem"><a href="../../cite.html">How to Cite the SEP</a></li>
                    <li role="menuitem"><a href="../../special-characters.html">Special Characters</a></li>
                    <li role="menuitem"><a href="../../tools/">Advanced Tools</a></li>
                    <li role="menuitem"><a href="../../contact.html">Contact</a></li>
                  </ul>
                </li>
                <li class="dropdown open"><a id="drop3" href="#" class="dropdown-toggle" data-toggle="dropdown" role="button"><i class="icon-leaf"></i> Support SEP</a>
                  <ul class="dropdown-menu" role="menu" aria-labelledby="drop3">
                    <li role="menuitem"><a href="../../support/">Support the SEP</a></li>
                    <li role="menuitem"><a href="../../support/friends.html">PDFs for SEP Friends</a></li>
                    <li role="menuitem"><a href="../../support/donate.html">Make a Donation</a></li>
                    <li role="menuitem"><a href="../../support/sepia.html">SEPIA for Libraries</a></li>
                  </ul>
                </li>
              </ul>
            </div>
          </div>
        </div>
      </div>
    </div>
    <!-- End navigation -->
    
    <div id="search">
      <form id="search-form" method="get" action="../../search/searcher.py">
        <input type="search" name="query" placeholder="Search SEP" />
        <div class="search-btn-wrapper"><button class="btn search-btn" type="submit" aria-label="search"><i class="icon-search"></i></button></div>
      </form>
    </div>
    <!-- End search --> 
    
  </div>
  <!-- End header --> 
</div>
<!-- End header wrapper -->

<div id="content">

<!-- Begin article sidebar -->
<div id="article-sidebar" class="sticky">
  <div class="navbar">
    <div class="navbar-inner">
      <div class="container">
        <button class="btn btn-navbar" data-target=".collapse-sidebar" data-toggle="collapse" type="button"> <i class="icon-reorder"></i> Entry Navigation </button>
        <div id="article-nav" class="nav-collapse collapse-sidebar in collapse">
          <ul class="nav">
            <li><a href="#toc">Entry Contents</a></li>
            <li><a href="#Bib">Bibliography</a></li>
            <li><a href="#Aca">Academic Tools</a></li>
            <li><a href="https://leibniz.stanford.edu/friends/preview/epistemic-utility/">Friends PDF Preview <i class="icon-external-link"></i></a></li>
            <li><a href="https://plato.stanford.edu/cgi-bin/encyclopedia/archinfo.cgi?entry=epistemic-utility">Author and Citation Info <i class="icon-external-link"></i></a> </li>
            <li><a href="#pagetopright" class="back-to-top">Back to Top <i class="icon-angle-up icon2x"></i></a></li>
          </ul>
        </div>
      </div>
    </div>
  </div>
</div>
<!-- End article sidebar --> 

<!-- NOTE: Article content must have two wrapper divs: id="article" and id="article-content" -->
<div id="article">
<div id="article-content">

<!-- BEGIN ARTICLE HTML -->


<div id="aueditable"><!--DO NOT MODIFY THIS LINE AND ABOVE-->

<h1>Epistemic Utility Arguments for Epistemic Norms</h1><div id="pubinfo"><em>First published Fri Sep 23, 2011; substantive revision Wed Oct 11, 2023</em></div>

<div id="preamble">

<p>
If I believe George Eliot wrote more than six novels and also believe
she wrote fewer than four, you will no doubt judge my beliefs
irrational. After all, if she did write more than six, she
didn&rsquo;t write fewer than four. And similarly, if my degree of
belief that Eliot wrote more than six novels is greater than my degree
of belief that she wrote more than five, then again you will think me
irrational. After all, if she did write more than six, she also wrote
more than five. In both cases, there is a general norm I fail to
satisfy. In the first, it might be this:</p>

<blockquote>

<p id="consistency-2">
<strong>Consistency\(_2\)</strong> If two propositions cannot both be
true together, you should not believe both.</p>
</blockquote>

<p>
In the second case, it might be this:</p>

<blockquote>

<p id="no-drop">
<strong>No Drop</strong> If one proposition entails another, then your
degree of belief in the first should not exceed your degree of belief
in the second.</p>
</blockquote>

<p>
This entry is concerned with norms like these. More specifically, it
is concerned with how we might establish these norms. More
specifically still, it is concerned with what epistemologists have
come to call <em>epistemic utility arguments</em> in their favor. The
norms we seek to establish primarily govern <em>epistemic states</em>;
that is, they say what rationality requires of your beliefs, degrees
of belief, and any other attitudes whose role it is to represent the
way the world is. But, as we will see, they also govern the activities
by which we acquire those states, such as gathering and responding to
evidence.</p>

<p>
Epistemic utility arguments are inspired by traditional utility-based
arguments in decision theory, so let&rsquo;s begin with a quick
summary of those. Traditional decision theory explores a particular
strategy for establishing the norms that govern which choices it is
rational for an individual to make in a particular situation (see
entry on
 <a href="../rationality-normative-utility/">normative theories of rational choice: expected utility</a>).
 Given such a situation, the framework for the theory includes:
<em>states of the world</em> described in as much detail as required;
<em>actions</em> that are available to the individual in the
situation, and the individual&rsquo;s <em>utility function</em>, which
takes a state of the world and an action and returns a measure of the
extent to which she values the outcome of performing that action at
that world. We call this measure the <em>utility</em> of the outcome
at the world. For example, there might be just two relevant states of
the world: one in which it rains and one in which it doesn&rsquo;t.
And there might be just two relevant actions from which to choose:
take an umbrella when you leave the house or don&rsquo;t. Then your
utility function will measure how much you value the outcomes of each
action at each state of the world: that is, it will give the value of
being in the rain without an umbrella, being in the rain with an
umbrella, being with an umbrella when there is no rain, and being
without an umbrella when there is no rain. With this framework in
hand, we can state certain very general norms of action in terms of
it. For instance, if one action has strictly greater utility than
another in every possible state of the world, we say the first
<em>strongly dominates</em> the second, and the norm of Dominance says
that you should not choose a strongly dominated action.</p>

<p>
In epistemic utility theory, the states of the world remain the same,
but the possible actions an individual might perform are replaced by
the possible <em>epistemic states</em> she might be in, and the
utility function is replaced by an <em>epistemic utility
function</em>, which takes a state of the world and an epistemic state
and returns a measure of the purely epistemic value of that epistemic
state at that state of the world. So, in epistemic utility theory, we
appeal to epistemic utility to ask which of a range of possible
epistemic states it is rational to be in, just as in traditional
utility theory we appeal to non-epistemic, pragmatic utility to ask
which of a range of possible actions it is rational to perform. (In
fact, we will often talk of epistemic <em>dis</em>utility rather than
epistemic utility in this entry. But it is easy to translate between
them: the negative of an epistemic utility function is an epistemic
disutility function, and <em>vice versa</em>.)</p>

<p>
Again, certain very general norms may be stated, such as the obvious
analogue of Dominance from above: if one epistemic state has greater
epistemic utility than another in every possible state of the world,
then it is irrational to be in the latter. And we might use them to
establish norms for epistemic states. For instance, consider
Consistency\(_2\) from the introduction. Let&rsquo;s say the epistemic
utility of believing a truth is some positive number \(R\), while the
epistemic utility of believing a falsehood is some negative number
\(-W\) (\(R\) for <em>R</em>ight, \(W\) for <em>W</em>rong). And
let&rsquo;s say the epistemic utility of a set of beliefs is just the
sum of the epistemic utilities of the individual beliefs that belong
to it, so that if you believe two truths and a falsehood, for
instance, your epistemic utility is \(2R - W\). Now suppose that \(W\)
is greater than \(R\); that is, the badness of believing falsely is
greater than the goodness of believing truly; that is, we more
strongly wish to avoid false belief than we want to achieve true
belief; that is, our epistemic utilities encode a sort of epistemic
conservatism. Then, if I believe each of two propositions that cannot
both be true together, my total epistemic utility is either \(R-W\),
if one is true and the other false, or \(-2W\) if neither is true. And
each of these is less than zero. So suspending judgment on both is
guaranteed to be better than believing both. And so, by Dominance,
believing both is irrational and we have an epistemic utility argument
for Consistency\(_2\). This is the style of argument we will consider
in this entry. As we&rsquo;ll see in
 <a href="#EpiUtiArgPro">Section 5.2</a>,
 there is an argument from Dominance to No Drop as well.</p>

<p>
Because we appeal to the purely <em>epistemic</em> utility of the
epistemic states we consider, rather than the <em>pragmatic</em> or
<em>practical</em> utility of the outcomes of the choices they lead us
to make, the arguments of epistemic utility theory are different from
betting arguments or dutch book arguments for epistemic norms (Ramsey
1926 [1931]; de Finetti 1937 [1980]; see entry on
 <a href="../dutch-book/">dutch book arguments</a>).
 They are also different from the sort of axiomatic justification
exemplified by Cox's theorem (Cox 1946, 1961; Paris 1994) as well as
the sort of structural justification given by R. I. G. Hughes and Bas
van Fraassen (1984) and Hannes Leitgeb (2021).</p>
</div> 

<div id="toc">
<!--Entry Contents-->

<ul>

<li><a href="#ModEpiSta">1. Modelling Epistemic States</a></li>
 
<li><a href="#ForArgEpiUtiThe">2. The Form of Arguments in Epistemic Utility Theory</a></li>
 
<li><a href="#AgeStaWor">3. Agendas and States of the World</a></li>
 
<li><a href="#EpiUtiArgFulBel">4. Epistemic Utility Arguments concerning Outright Beliefs</a>
 
 <ul>

 <li><a href="#DomCon">4.1 Dominance and Consistency</a></li>
 
 <li><a href="#ExpEpiUtiLocThe">4.2 Expected Epistemic Utility and the Lockean Thesis</a></li>
 
 <li><a href="#StrDomLocThe">4.3 Dominance and Almost Lockean Completeness</a></li>
 
 <li><a href="#UpBel">4.4 Updating your beliefs when you receive evidence</a></li>
 
 </ul></li>
 
<li><a href="#EpiUtiArgPreCre">5. Epistemic Utility Arguments for Precise Credences</a>
 
 <ul>

 <li><a href="#EpiUtiFunPreCre">5.1 Epistemic utility function for precise credences</a></li>
 
 <li><a href="#EpiUtiArgPro">5.2 Epistemic utility arguments for Probabilism</a></li>
 
 <li><a href="#EpiUtiArgChCr">5.3 Epistemic utility arguments for chance-credence norms</a></li>
 
 <li><a href="#EpiUtiArgCon">5.4 Epistemic utility arguments for Conditionalization</a></li>
 
 <li><a href="#EpiArgUniThe">5.5 Epistemic utility arguments for and against the Uniqueness Thesis</a></li>

 <li><a href="#EpiUtiArgSocEpi">5.6 Epistemic utility arguments in social epistemology</a></li>
 
 </ul></li>

<li><a href="#ComCon">6. Comparative confidence</a></li>

<li><a href="#ImpCre">7. Imprecise credences</a></li>
 

<li><a href="#Bib">Bibliography</a></li>

<li><a href="#Aca">Academic Tools</a></li>
 
<li><a href="#Oth">Other Internet Resources</a></li>
 
<li><a href="#Rel">Related Entries</a></li>

</ul>

<!--Entry Contents-->

<hr />
</div>

<div id="main-text">

<h2><a id="ModEpiSta">1. Modelling Epistemic States</a></h2>

<p>
Let&rsquo;s begin by listing the different ways in which we might
represent an individual&rsquo;s epistemic state at a given time (see
entry on
 <a href="../formal-belief/">formal representations of belief</a>).
 We might represent them using any of the following:</p>

<ul>

<li>the set of propositions she believes at the time (we might call
this the <em>outright belief model</em>; it is the object of study in
much traditional epistemology and in doxastic and epistemic logic;
outright beliefs are those we report when we say &lsquo;I think I
switched off the gas&rsquo; or &lsquo;I believe it will rain
tomorrow&rsquo;; we&rsquo;ll consider them in
 <a href="#EpiUtiArgFulBel">Section 4</a>);</li>
 
<li>a credence function, which takes each proposition about which she
has an opinion and returns her credence in that proposition at the
time, where her credence in a proposition measures how confident she
is in it as a number at least 0 and at most 1 (this is the <em>precise
credence</em> or <em>standard Bayesian model</em>; it is the object of
study in much formal and Bayesian epistemology; credences are the
states we report when we say &lsquo;I&rsquo;m 70&percnt; sure I
switched off the gas&rsquo; or &lsquo;I&rsquo;m 50-50 whether it will
rain tomorrow&rsquo;; we&rsquo;ll consider them in
 <a href="#EpiUtiArgPreCre">Section 5</a>);</li>
 
<li>a comparative confidence ordering, which takes each pair of
propositions about which she has an opinion and says whether she is
more confident in one than the other, equally confident in both, or
neither more confident in one nor equally confident in both (this is
the <em>comparative confidence model</em>; we report comparative
confidences when we say &lsquo;I&rsquo;m more confident I switched off
the gas than the electricity&rsquo; or &lsquo;I&rsquo;m more confident
than not that it will rain tomorrow&rsquo;; we&rsquo;ll consider them
in
 <a href="#ComCon">Section 6</a>);</li>
 
<li>a set of credence functions, each of which is a precisification of
her otherwise vague or imprecise or indeterminate credences at the
time (this is the <em>imprecise credence model</em>; we report
imprecise credences when we say &lsquo;I&rsquo;m
50&percnt;&ndash;70&percnt; sure I switched off the gas&rsquo; or
&lsquo;I&rsquo;m less than 60&percnt; sure it will rain
tomorrow&rsquo;; we&rsquo;ll consider them in
 <a href="#ImpCre">Section 7</a>);</li>
 </ul>

<p>
Epistemic utility theory may be applied to any one of these ways of
representing an individual&rsquo;s epistemic state. Whichever we
choose, we define an epistemic (dis)utility function to be a function
that takes a set of epistemic states modelled in this way, together
with a state of the world, to a real number, or (positive or) negative
infinity, and we take this number to measure the epistemic
(dis)utility of those epistemic attitudes at that world.</p>

<h2><a id="ForArgEpiUtiThe">2. The Form of Arguments in Epistemic Utility Theory</a></h2>

<p>
We begin by highlighting the form that epistemic utility arguments
take, regardless of which sorts of epistemic states concern us. Recall
the argument for
 <a href="#consistency-2">Consistency\(_2\)</a>
 from the introduction. It had two premises: first, we placed
conditions on the measure of epistemic utility; second, we stated a
norm from standard utility theory. From those, we deduced the norm on
belief. This is the structure of nearly all epistemic utility
arguments. In general, in epistemic utility theory, we argue for an
epistemic norm N using the following two ingredients:</p>

<ul>

<li><strong>E</strong> A set of conditions that a legitimate measure
of epistemic utility must satisfy.</li>

<li><strong>Q</strong> A norm of standard utility theory (or decision
theory), which is to be applied, using an epistemic utility function,
to discover what rationality requires of an agent in a given
situation.</li>
</ul>

<p>
Typically, the inference from E and Q to N appeals to a mathematical
theorem, which shows that, applied to any epistemic utility function
that satisfies the conditions E, the norm Q entails the norm N.</p>

<h2><a id="AgeStaWor">3. Agendas and States of the World</a></h2>

<p id="agenda">
One final piece of general stage-setting will be useful before we can
embark on our journey through the arguments. On each of the models of
belief listed in
 <a href="#ModEpiSta">Section 1</a>,
 there is a set of propositions with which our individual&rsquo;s
epistemic states are concerned: the ones to which they assign
credences in the precise credence model, for instance. We call this
set their <em>agenda</em> and often denote it \(\mathcal{F}\).</p>

<p>
For the most part, we&rsquo;ll assume this agenda is a finite algebra
of propositions. That is, (i) it contains finitely many propositions;
(ii) it contains a necessarily true proposition and a necessarily
false proposition; (iii) and it is closed under taking negations,
conjunctions, and disjunctions&mdash;that is, if it contains a
proposition, it also contains its negation, and if it contains two
propositions, it also contains their conjunction and their
disjunction. (But see
 <a href="#InfProSpa">Section 5.2.5</a>,
 where we lift assumption (i).)</p>

<p id="worlds">
The epistemic utility of an individual&rsquo;s set of epistemic
attitudes depends on the way the world is. For instance, a belief is
epistemically valuable if it&rsquo;s true, but disvaluable if
it&rsquo;s false. So we&rsquo;d better say how we represent these
states of the world formally. For the most part, we&rsquo;ll assume
the logic that governs the propositions in our individual&rsquo;s
agenda is classical (but see
 <a href="#NonClaLog">Section 5.2.6</a>,
 where we drop that assumption). In that case, a state of the world
relative to an agenda is just a classically consistent assignment of
the classical truth values, True and False, to the propositions in
that agenda. We often write \(\mathcal{W}_\mathcal{F}\) for the set of
states of the world relative to the agenda \(\mathcal{F}\).</p>

<p>
Since \(\mathcal{F}\) is a finite algebra, for each state of the world
\(w\) relative to \(\mathcal{F}\), there is a proposition in
\(\mathcal{F}\) that is true at that state of the world and only at
that state of the world. We will abuse notation and also write \(w\)
for this proposition.</p>

<h2><a id="EpiUtiArgFulBel">4. Epistemic Utility Arguments concerning Outright Beliefs</a></h2>

<p>
In the outright belief model, our individual&rsquo;s epistemic state
is represented by their <em>belief set</em>, which contains each
proposition in their agenda that they believe; on the rest, we say
that they suspend judgment.</p>

<p id="jamesian">
Following Kenny Easwaran (2016) and Kevin Dorst (2017), who in turn
generalize Carl Hempel&rsquo;s (1962) approach, we define the
epistemic utility of a belief set at a state of the world as follows:
the epistemic utility of a belief in a true proposition is \(R\),
while the epistemic utility of a belief in a false proposition is
\(-W\), where \(0 \lt R, W\); and the epistemic utility of a belief
set is the sum of the epistemic utilities of the beliefs it contains.
I&rsquo;ll call this the Jamesian Epistemic Utility assumption, since
it allows for different ways of scoring the goodness of believing a
truth and the badness of believing a falsehood, which goes some way to
explicating the view in William James&rsquo; &lsquo;The Will to
Believe&rsquo; (1897).</p>

<p>
Easwaran and Dorst impose no constraints on \(R\) and \(W\) except
\(-W \lt 0 \lt R\). Hempel assumes \(R, W = 1\).</p>

<h3><a id="DomCon">4.1 Dominance and Consistency</a></h3>

<p>
So, with all this in hand, the argument for Consistency\(_2\) runs as
follows:</p>

<blockquote>

<p id="weak-single-premise-closure">
<strong>Epistemic Conservatism</strong> \(W \gt R \gt 0\).</p>

<p id="dom">
<strong>Dominance</strong> If one option strongly dominates another,
rationality requires you not to choose the second.</p>

<p>
Therefore,</p>

<p>
<strong>Consistency\(_2\)</strong> If two propositions cannot both be
true together, rationality requires that you should not believe
both.</p>
</blockquote>

<h3><a id="ExpEpiUtiLocThe">4.2 Expected Epistemic Utility and the Lockean Thesis</a></h3>

<p>
Dominance is a very weak norm of rational choice, but there are
stronger ones. Among them is one of the most popular norms of decision
theory, which says you should maximize expected utility from the point
of view of your credences.</p>

<blockquote>

<p id="max-sub-exp-ut">
<strong>Maximize Subjective Expected Utility</strong> If one option
has greater expected utility than another from the point of view of
your credences, then rationality requires that you should not choose
the second.</p>
</blockquote>

<p>
The expected utility of an option relative to your credences is
obtained by taking its utility at each state, weighting that by your
credence in that state, and summing up these credence-weighted
utilities. For instance, suppose \(p\) is your credence in a
proposition and \(1-p\) is your credence in its negation. Then the
expected epistemic utility of believing that proposition from the
point of view of your credences is</p>

<blockquote>
\(p \times \text{epistemic utility of believing a truth} + (1-p)\
\times\)
<br />
\( \text{epistemic utility of believing a falsehood} = pR +
(1-p)(-W)\)
</blockquote>

<p>
Now we can ask when believing a proposition maximizes expected utility
from the point of view of your credences.</p>

<blockquote>

<p id="exp-thm-lock">
<strong>Expectation Theorem for the Lockean Thesis (Hempel 1962;
Easwaran 2016; Dorst 2017)</strong> Suppose your credence in \(X\) is
\(p\). Then:</p>

<ul>

<li>If \(p \gt \frac{W}{R+W}\), then believing \(X\) uniquely
maximizes expected epistemic utility. (That is, believing has strictly
greater expected utility than suspending.)</li>

<li>If \(p = \frac{W}{R+W}\), then believing \(X\) and suspending
judgment on \(X\) both maximize expected epistemic utility. (That is,
believing has the same expected utility as suspending.)</li>

<li>If \(p \lt \frac{W}{R + W}\), then suspending judgment on \(X\)
uniquely maximizes expected epistemic utility. (That is, believing has
strictly less expected utility than suspending.)</li>
</ul>
</blockquote>

<p>
Easwaran and Dorst both appeal to this result to argue for the Lockean
Thesis, which is the most well-known norm that connects credences and
outright beliefs. It takes its name from passages in John
Locke&rsquo;s (1689 [1975]) <em>An Essay on Human Understanding</em>
in which he suggests that belief is nothing more than sufficiently
high credence, but it was formulated explicitly by Richard Foley
(1992). In fact, the Lockean Thesis is a family of putative norms:
each member of the family is distinguished by the threshold above
which it demands belief, below which it demands suspension, and at
which it permits either.</p>

<blockquote>

<p id="Lockean-t">
<strong>The Lockean Thesis with threshold \(t\)</strong></p> Suppose
your credence in \(X\) is \(p\). Then

<ul>

<li>If \(p \gt t\), you are rationally required to believe \(X\);</li>

<li>If \(p = t\), you are rationally permitted to believe \(X\) and
rationally permitted to suspend on \(X\);</li>

<li>If \(p \lt t\), you are rationally required to suspend on
\(X\).</li>
</ul>
</blockquote>

<p>
Then we have the following argument:</p>

<blockquote>

<p id="lockean-thesis-arg">
 <strong><a href="#jamesian">Jamesian Epistemic Utility</a></strong>
 \(0 \lt R, W\).</p>

<p>
 <strong><a href="#max-sub-exp-ut">Maximize Subjective Expected Utility</a></strong></p>
 
<p>
Therefore,</p>

<p>
 <strong><a href="#Lockean-t">The Lockean Thesis with threshold \(\frac{W}{R+W}\)</a></strong></p>
 </blockquote>

<h4><a id="ExpEpiUtiUni">4.2.1 Expected Epistemic Utility, the Uniqueness Thesis, and Epistemic Permissivism</a></h4>

<p>
Thomas Kelly (2014) has pointed out that this argument has an
interesting consequence for the debate in epistemology between those
who argue for the Uniqueness Thesis and those who side with Epistemic
Permissivism. Given a sort of epistemic state, the Uniqueness Thesis
for that sort of state runs as follows:</p>

<blockquote>

<p id="uniqueness-thesis">
<strong>The Uniqueness Thesis</strong> Given an agenda, the following
holds: for any body of evidence, there is a unique epistemic state of
the given sort concerning the propositions in that agenda that
rationality requires you to have if that body of evidence is your
total evidence.</p>
</blockquote>

<p>
Epistemic Permissivism is simply the negation of the Uniqueness
Thesis. So, for instance, the Uniqueness Thesis for credences says
that, for any agenda and any body of evidence, there is a unique
credence function over that agenda that rationality requires you to
have in response to that evidence, while Epistemic Permissivism says
that there is at least one body of evidence for which there is more
than one credence function it is rational to have in response to that
evidence.</p>

<p>
Now, suppose you subscribe to the Uniqueness Thesis about credences.
But you also think that rationality permits different ways in which
you might set the utility \(R\) of believing a truth and the utility
\(-W\) of believing a falsehood. For instance, you might think \(R =
1, W = 2\) is permitted, but so is \(R = 2, W = 3\). And suppose the
unique rational credence in proposition \(X\) is 0.65. Maximizing
expected epistemic utility with \(R = 2, W = 3\) entails the Lockean
Thesis with threshold \(\frac{3}{2+3}\), and this requires that you
believe \(X\), since \(0.65 \gt \frac{3}{2+3}\); but maximizing
expected epistemic utility with \(R = 1, W = 2\) entails the Lockean
Thesis with threshold \(\frac{2}{1+2}\), and this requires that you
suspend on \(X\), since \(0.65 \lt \frac{2}{1+2}\). And so we have
Epistemic Permissivism about outright beliefs, even though we have the
Uniqueness Thesis for credences. Which beliefs rationality requires
you to have depends not only on your evidence, but also on your
epistemic utilities. In general, the smaller the ratio of \(R\) to
\(W\), the higher your credence in a proposition must be for you to
believe it.</p>

<h4><a id="ExpEpUtCon">4.2.2 Expected Epistemic Utility and Consistency</a></h4>

<p>
Another interesting consequence of the
 <a href="#exp-thm-lock">Expectation Theorem for the Lockean Thesis</a>
 is that it shows that we can&rsquo;t give an argument from
 <a href="#dom">Dominance</a>
 to the natural generalization of
 <a href="#consistency-2">Consistency\(_2\)</a>
 to arbitrarily many propositions; and we can only give the
generalizations to \(n\) propositions, for some given \(n\), by
becoming increasingly conservative in our epistemic utilities
(Fitelson &amp; Easwaran 2015; Easwaran 2016).</p>

<p>
Say that a set of propositions is inconsistent if the propositions in
it cannot all be true together. Then we have the following two
norms:</p>

<blockquote>

<p id="n-consistency">
<strong>Consistency\(_n\)</strong> If <strong>A</strong> is an
inconsistent set of \(n\) or fewer propositions, then rationality
requires that you should not believe each proposition in
<strong>A</strong>.</p>
</blockquote>

<blockquote>

<p id="full-consistency">
<strong>Consistency</strong> If <strong>A</strong> is an inconsistent
set of finitely many propositions, then rationality requires that you
should not believe each proposition in <strong>A</strong>.</p>
</blockquote>

<p>
Take any \(n\) and suppose there is a lottery with \(n\) tickets in
which each ticket is equally likely to be the winner. Let \(L_i\) be
the proposition that says the \(i^\mathrm{th}\) ticket will lose. Then
the set of propositions \(\{L_1, \ldots, L_n\}\) is inconsistent.
Nonetheless, if my credence in each \(L_i\) is \(1 - \frac{1}{n}\), as
it should be, and the Lockean threshold \(t\) is below \(1 -
\frac{1}{n}\), then the
 <a href="#Lockean-t">Lockean Thesis</a>
 with threshold \(t\) says I must believe each member of this
incoherent set. What&rsquo;s more, we know that believing each
maximizes expected epistemic utility from the point of view of those
credences, providing \(\frac{W}{R + W}\) is less than \(1 -
\frac{1}{n}\). And we also know that no strongly dominated option can
ever maximize expected utility. So, if \(W \lt (n-1)R\), and therefore
\(\frac{W}{R + W} \lt 1 - \frac{1}{n}\), then it&rsquo;s possible to
have an inconsistent set of beliefs in \(n\) propositions that is not
dominated. And this shows in turn that there can be no argument from
 <a href="#dom">Dominance</a>
 to
 <a href="#full-consistency">Consistency</a>.
 For any \(W \gt R \gt 0\), there is some \(n\) such that
\(\frac{W}{R+W} \lt 1 - \frac{1}{n}\). On the other hand, if \(n \lt
\frac{R+W}{R}\), then we can at least argue from
 <a href="#dom">Dominance</a>
 to
 <a href="#n-consistency">Consistency\(_n\)</a>,
 since the maximum epistemic utility of believing each of \(n\)
inconsistent propositions is \((n-1)R - W\), which is always less than
zero if \(n \lt \frac{R+W}{R}\).</p>

<h3><a id="StrDomLocThe">4.3 Dominance and Almost Lockean Completeness</a></h3>

<p>
Daniel Rothschild (2021) gives an argument from
 <a href="#dom">Dominance</a>
 to a norm he calls Almost Lockean Completeness for Belief Sets at a
threshold, which says:</p>

<blockquote>

<p id="lockean-completeness">
<strong>Almost Lockean Completeness for Belief Sets with threshold
\(t\)</strong> Rationality requires that there there is some
probabilistic credence function such that your beliefs satisfy the
Lockean Thesis with threshold \(t\) with respect to that credence
function.</p>
</blockquote>

<p>
It&rsquo;s worth noting that the Lockean Thesis and Almost Lockean
Completeness for Belief Sets are quite different norms. The first
governs a relationship between your credences and your beliefs; the
second does not even assume you have any credences, and instead
governs only your beliefs&mdash;it says that your beliefs should be
<em>as they would be if</em> you were to have credences and they were
to be related to your beliefs as the Lockean Thesis says they should
be.</p>

<p id="uni-rat">
Here is Rothschild&rsquo;s argument. Above, we assumed that the
epistemic utilities of true beliefs in different propositions are the
same, and similarly for false beliefs. But we might score attitudes to
different propositions differently: \(R_X\) for a true belief in
\(X\), \(-W_X\) for a false belief in \(X\); \(R_Y\) for a true belief
in \(Y\), \(-W_Y\) for a false belief in \(Y\); and so on. For
instance, if \(X\) concerns the correct fundamental theory of physics,
I might set \(R_X = 10, W_X = 20\), while if \(Y\) concerns the number
of blades of grass in Hyde Park, I might set \(R_Y = 1, W_Y = 2\).
This might reflect the greater importance of \(X\) than \(Y\). Now, if
\(\frac{W_X}{R_X} = \frac{W_Y}{R_Y}\), for any two propositions \(X,
Y\), we say that the epistemic utilities have <em>uniform ratio</em>.
And in that case the Lockean threshold is the same for each
proposition if we maximize expected utility. But what about
 <a href="#dom">Dominance</a>?
 Rothschild proves the following result:</p>

<blockquote>

<p id="rothschild-proposition">
<strong>Dominance Theorem for Almost Lockean Completeness for Belief
Sets (Rothschild 2021)</strong> A belief set satisfies Almost Lockean
Completeness iff there is no set of epistemic utilities with uniform
ratio under which the pair is strongly dominated.</p>
</blockquote>

<p>
I sketch the proof in the
 <a href="maths-inacc.html#DomTheAlmLoc">supplementary materials</a>.</p>
 
<p>
One issue with this argument: To extract
 <a href="#lockean-completeness">Almost Lockean Completeness</a>
 from the
 <a href="#rothschild-proposition">Dominance Theorem for Almost Lockean Completeness</a>,
 a decision-theoretic norm is needed. It would say that it&rsquo;s
irrational to be dominated relative to <em>any</em> set of epistemic
utilities that have uniform ratio. But this norm is not compelling.
Why care about being dominated relative to epistemic utilities that
are not your own?</p>

<h3><a id="UpBel">4.4 Updating your beliefs when you receive evidence</a></h3>

<p>
All the norms we have considered so far in this entry have been
synchronic: that is, they say what rationality requires of your
beliefs at a given time. In this section, we turn to diachronic norms:
these say what rationality requires of the relationship between your
beliefs at one time and your beliefs at another, typically when you
learn some evidence between those times. We&rsquo;ll look at two sets
of norms for belief updating: AGM belief revision
 (<a href="#AGMBelRev">Section 4.4.1</a>)
 and Plan Lockean Revision
 (<a href="#PlaLocThe">Section 4.4.2</a>).</p>
 
<h4><a id="AGMBelRev">4.4.1 AGM belief revision</a></h4>

<p>
AGM belief revision is a set of putative norms that govern full
beliefs introduced by Carlos Alchourr&oacute;n, Peter G&auml;rdenfors,
and David Makinson (1985); it includes both synchronic and diachronic
norms (see entry on
 <a href="../logic-belief-revision/">the logic of belief revision</a>).
 The synchronic norms are these, and they apply both before and after
you acquire new evidence:</p>

<blockquote>

<p id="agm-consistency">
<strong>Consistency</strong> If \(\mathbf{A}\) is an inconsistent set
of finitely many propositions, then you should not believe each
proposition in \(\mathbf{A}\).</p>

<p id="agm-closure">
<strong>Closure</strong> If \(X\) is a logical consequence of the
propositions in \(\mathbf{A}\), then you should not believe each
proposition in \(\mathbf{A}\) while not believing \(X\).</p>
</blockquote>

<p>
We&rsquo;ve already seen that we cannot give an argument for either of
these from the point of view of epistemic utility. But perhaps we can
do better with the diachronic norms. These norms govern a belief
updating operator \(\star\), which takes your belief set
\(\mathbf{B}\), together with a proposition \(E\) that gives the
evidence you learn, and returns \(\mathbf{B} \star E\), which is your
new belief set after you learn \(E\). The AGM postulates state that
the following are required by rationality:</p>

<blockquote>

<p id="agm-success">
<strong>Success</strong> \(E\) is in \(\mathbf{B} \star E\). That is,
after learning a proposition, you should believe it.</p>

<p id="agm-inclusion">
<strong>Inclusion</strong> \(\mathbf{B} \star E \subseteq
\mathsf{Cn}(\mathbf{B} \cup \{E\})\), where \(\mathsf{Cn}\) takes a
set of propositions and returns its logical closure. That is, you
should believe a proposition after learning only if it is a logical
consequence of what previously believed and the proposition you
learn.</p>

<p id="agm-preservation">
<strong>Preservation</strong> If \(\mathbf{B}\) and \(E\) are
consistent, then \(\mathbf{B} \subseteq \mathbf{B} \star E\). That is,
if what you learn is consistent with what you previously believed, you
shouldn&rsquo;t drop your belief in anything you previously believed
when you learn.</p>

<p id="agm-extensionality">
<strong>Extensionality</strong> If \(E\) and \(F\) are logically
equivalent, then \(\mathbf{B} \star E = \mathbf{B} \star F\). That is,
learning either of two logically equivalent propositions should change
your beliefs in the same way.</p>
</blockquote>

<p>
As Shear and Fitelson (2019) show, we can justify
 <a href="#agm-success">Success</a>,
 <a href="#agm-inclusion">Inclusion</a>, and
 <a href="#agm-extensionality">Extensionality</a>
 by appealing to
 <a href="#max-sub-exp-ut">Maximize Expected Epistemic Utility</a>.
 To do this, we note that, as we&rsquo;ll justify in
 <a href="#EpiUtiArgCon">Section 5.3</a>,
 there is a standard norm that is taken to govern how we should update
our credences in response to new evidence. It&rsquo;s called
Conditionalization and it says that, if you assign positive credence
to a proposition \(E\) and then learn \(E\) for sure as evidence, then
your new credence in a proposition \(X\) should be your old
conditional credence in \(X\) given \(E\), where that is defined to be
the ratio of your credence in the conjunction of \(X\) and \(E\) to
your credence in \(E\) alone; or, in other words, it&rsquo;s the
proportion of your old credence in \(E\) that you also assigned to
\(X\). So you might think that (i) you should set your original
credences by maximising expected utility with respect to your original
credences, and (ii) set your new credences after learning evidence by
maximising expected utility with respect to your new credences, which
are obtained from your old ones by updating on your evidence in line
with Conditionalization. Doing this secures
 <a href="#agm-success">Success</a>,
 <a href="#agm-inclusion">Inclusion</a>, and
 <a href="#agm-extensionality">Extensionality</a>.
 It doesn&rsquo;t secure
 <a href="#agm-preservation">Preservation</a>
 because learning a new proposition that is consistent with all the
propositions to which you assigned credence higher than some threshold
can lead you to drop your credence in one of those propositions below
that threshold.</p>

<h4><a id="PlaLocThe">4.4.2 Almost Lockean Completeness for Updating Plans</a></h4>

<p>
Patrick Rooyakkers (ms) has extended Rothschild&rsquo;s argument for
the Lockean Thesis so that it establishes a version of Almost Lockean
Completeness but for combinations of belief sets and updating plans.
The idea is this: Suppose you&rsquo;re about to learn some new
evidence. You don&rsquo;t know what it is, but you know it will be one
of the propositions \(E_1, \ldots, E_k\), which together form a
partition&mdash;that is, the propositions are exhaustive and mutually
exclusive. Then, as well as your prior belief set, which we&rsquo;ll
call \(\mathbf{B}\), you should have an updating plan in place for how
you&rsquo;ll respond to each of the possible pieces of evidence you
might receive; for each \(E_i\), this should give the belief set
\(\beta_{E_i}\) you plan to have if \(E_i\) is the proposition you
learn. Then the following norm governs this plan:</p>

<blockquote>

<p id="plan-lockean-thesis">
<strong>Almost Lockean Completeness for Plans with threshold
\(t\)</strong> Rationality requires that there is a probabilistic
credence function such that:</p>

<ol type="i">

<li>If the unconditional credence it assigns to \(X\) is greater than
\(t\), then \(X\) is in \(\mathbf{B}\); if it is less than \(t\), then
\(X\) is not in \(\mathbf{B}\); </li>

<li>For each \(E_i\), if the conditional credence it assigns to \(X\)
given \(E_i\) is greater than \(t\), then \(X\) is in \(\beta_{E_i}\);
if it is less than \(t\), then \(X\) is not in \(\beta_{E_i}\).</li>
</ol>
</blockquote>

<p>
That is, you are required to obey the Lockean Thesis with a particular
threshold and some probabilistic credence function before you acquire
the new evidence, and obey it with respect to some unconditional
credences; and you are required to plan to obey it with the same
threshold after you&rsquo;ve received the evidence, and obey it then
with respect to the conditional credences given the evidence you
learn.</p>

<p>
Rooyakkers&rsquo; argument has the same form as Rothschild&rsquo;s. He
proves the following on the assumption that the epistemic utility at a
world of a pair consisting of a prior belief set and a belief plan
should be the sum of the epistemic utility of the prior belief set at
that world and the epistemic utility of the posterior belief set that
the plan endorses if you learn the proposition in the partition that
is true at that world:</p>

<blockquote>

<p id="rooyakkers-proposition">
<strong>Dominance Theorem for Almost Lockean Completeness for Plans
(Rooyakkers ms)</strong> A prior belief set and a belief plan together
satisfy Almost Lockean Completeness for Plans iff there is no set of
epistemic utilities with uniform ratio under which the pair is
strongly dominated.</p>
</blockquote>

<h2><a id="EpiUtiArgPreCre">5. Epistemic Utility Arguments for Precise Credences</a></h2>

<p>
In the precise credence model, we represent an individual&rsquo;s
epistemic state by their <em>credence function</em>, which takes each
proposition in their agenda and returns a real number at least 0 and
at most 1, which measures the strength of their belief, or their
degree of confidence, in that proposition. In mathematical notation,
\(C : \mathcal{F} \rightarrow [0, 1]\).</p>

<p>
We&rsquo;ll meet the norm of Probabilism below, but it will be helpful
to say here what it means for a credence function to be probabilistic,
so that we can use that notion in the coming section. Suppose \(C\) is
a credence function defined on the agenda \(\mathcal{F}\). Then, if
\(\mathcal{F}\) is an algebra, <em>\(C\) is probabilistic</em> if</p>

<ul>

<li>Normality: \(C(\top) = 1\), if \(\top\) is a tautology; and
\(C(\bot) = 0\), if \(\bot\) is a contradiction.</li>

<li>Finite Additivity: \(C(X \vee Y) = C(X) + C(Y)\), for all mutually
exclusive \(X\) and \(Y\) in \(\mathcal{F}\).</li>
</ul>

<p id="mixture">
Another piece of terminology that will be useful below. A
<em>mixture</em> of a sequence of credence functions is a weighted sum
of them. That is, given a sequence of credence functions, \(C_1,
\ldots, C_n\) and a sequence of weights \(0 \leq \lambda_1, \ldots,
\lambda_n \leq 1\) that sum to 1, we define the mixture of these
credence functions with these weights to be \(\lambda_1C_1 + \ldots +
\lambda_nC_n\), where, for each \(X\), 
\[(\lambda_1C_1 + \ldots + \lambda_nC_n)(X) = \lambda_1C_1(X) + \ldots + \lambda_nC_n(X)\]
 The <em>straight
mixture</em> of \(C_1, \ldots, C_n\) is the mixture in which each
receives the same weight. If each of a sequence of credence functions
is probabilistic, so is any mixture of them.</p>

<h3><a id="EpiUtiFunPreCre">5.1 Epistemic utility functions for precise credences</a></h3>

<p id="sr">
An epistemic utility function for the states considered in the precise
credence model takes a credence function on an agenda and a state of
the world relative to that same agenda, and returns a real number or
\(-\infty\) or \(\infty\) that measures the epistemic utility of that
credence function at that state of the world. One of the most popular
epistemic utility functions is the <em>Brier score</em>. To define it,
we first define a measure of the epistemic utility of a single
credence in a single proposition, and then use that to generate a
measure of the epistemic utility of an entire credence function on an
agenda. Measures of the epistemic utility of a single credence are
known as <em>scoring rules</em>. Suppose \(s\) is a scoring rule.
Then, given a credence \(p\):</p>

<ul>

<li>\(s(1, p)\) gives the epistemic utility of having credence \(p\)
in a true proposition;</li>

<li>\(s(0, p)\) gives the epistemic utility of having credence \(p\)
in a false proposition.</li>
</ul>

<p id="quad-sr">
Here&rsquo;s the <em>quadratic scoring rule</em>:</p>

<ul>

<li>\(q(1, p) = -(1-p)^2\)</li>

<li>\(q(0, p) = -p^2\)</li>
</ul>

<p>
One way to understand this: If a proposition is true, then credence 1
is the ideal credence in it; if it&rsquo;s false, then credence 0 is
the ideal. The quadratic scoring rule says that the epistemic
<em>dis</em>utility of a credence is the square of the difference
between it and the ideal, and the epistemic utility is the negative of
that. So the epistemic utility of a credence is its proximity to the
ideal credence for a particular way of measuring distance.</p>

<p id="Brier">
The <em>Brier score</em> of an entire credence function is then
defined to be the sum of the quadratic scores of the credences it
assigns. So: 
\[
B(C, w) = \sum_{X \in \mathcal{F}} q(V_w(X), C(X)) = -\sum_{X \in \mathcal{F}} (V_w(X) - C(X))^2
\]
 where \(V_w(X) = 1\) if \(X\) is true at
\(w\), and \(V_w(X) = 0\) if \(X\) is false at \(w\). We might think
of \(V_w\) as the ideal or omniscient credence function at world
\(w\).</p>

<p>
The quadratic score and the Brier score have certain properties that
are important in epistemic utility arguments concerning precise
credences:</p>

<p>
First, properties of scoring rules:</p>

<blockquote>

<p id="continuity-sr">
<strong>Continuity (for scoring rules)</strong> We say that a scoring
rule \(s\) is <em>continuous</em> if \(s(1, p)\) and \(s(0, p)\) are
both continuous functions of \(p\) on \([0, 1]\).</p>

<p id="strict-propriety-sr">
<strong>Strict Propriety (for scoring rules)</strong> We say that a
scoring rule \(s\) is <em>strictly proper</em> if, for any \(0 \leq p
\leq 1\), 
\[
ps(1, x) + (1-p)s(0, p)
\]
 is maximized uniquely at \(x = p\). (That is,
each probability expects itself to be best, epistemically
speaking.)</p>
</blockquote>

<p>
Second, properties of epistemic utility functions:</p>

<blockquote>

<p id="continuity-eum">
<strong>Continuity (for epistemic utility functions)</strong> We say
that an epistemic utility measure \(EU\) is <em>continuous</em> if
\(EU(C, w)\) is a continuous function of \(C\) on the set of credence
functions defined on the same agenda.</p>

<p id="strict-propriety-eum">
<strong>Strict Propriety (for epistemic utility functions)</strong> We
say that an epistemic utility measure \(EU\) is <em>strictly
proper</em> if, for any probabilistic credence function \(P\) defined
on \(\mathcal{F}\), 
\[
\sum_{w \in \mathcal{W}_\mathcal{F}} P(w)EU(C, w)
\]
 is maximized uniquely, among credence
functions defined on \(\mathcal{F}\), at \(C = P\). (That is, each
probabilistic credence function expects itself to be best,
epistemically speaking.)</p>

<p id="extensionality-eum">
<strong>Extensionality (for epistemic utility functions)</strong> We
say that an epistemic utility measure \(EU\) is <em>extensional</em>
if, whenever \(C\) is a probability function on \(\mathcal{F}\), and
\(\pi\) is a permutation of the worlds in \(\mathcal{W}_\mathcal{F}\),
then \(EU(C, w) = EU(\pi(C), \pi(w))\), where \(\pi(C)(w) =
C(\pi(w))\).</p>

<p id="additivity-eum">
<strong>Additivity (for epistemic utility functions)</strong> We say
that an epistemic utility measure \(EU\) is <em>additive</em> if, for
each \(X\) in \(\mathcal{F}\), there is a scoring rule \(s_X\) such
that</p> 
\[
EU(C, w) = \sum_{X \in \mathcal{F}} s_X(V_w(X), C(X))
\]

<p>
(That is, the epistemic utility of an entire credence function is the
sum of the epistemic utilities of the individual credences, where
these can be given by different scoring rules for each proposition in
the agenda.)</p>
</blockquote>

<p>
Suppose \(EU\) is additive. Then (i) \(EU\) is continuous iff each
\(s_X\) is continuous; and (ii) \(EU\) is strictly proper iff each
\(s_X\) is strictly proper.</p>

<p>
A detailed discussion of the various arguments for using scoring rules
and epistemic utility functions that have these properties claims can
be found in the
 <a href="char-ep-ut-cred.html">supplementary materials</a>.
 They roughly divide into two sorts:</p>

<p>
The first sort of argument is based on a veritist account of epistemic
value, which says that the sole fundamental source of epistemic value
is what Joyce (1998) calls <em>gradational accuracy</em>. The idea is
that a credence in a true proposition is more accurate, and therefore
more valuable, epistemically speaking, the higher it is, while a
credence in a false proposition is more accurate, and thus
epistemically more valuable, the lower it is. Put another way, the
ideal credence function to have at a world is the omniscient credence
function at that world, which assigns maximal credence to all the
truths and minimal credence to all the falsehoods; and your credence
function is more accurate, and therefore epistemically more valuable,
the closer it lies to this ideal. So, for veritists, epistemic utility
functions measure the accuracy of a credence function, and
characterizing the legitimate epistemic utility functions is
characterizing the legitimate ways of measuring accuracy.</p>

<p>
Joyce (1998) offers one such characterization, based on a series of
axioms he justifies individually. Leitgeb and Pettigrew (2010) offer
an argument for the Brier score based on the requirement to avoid a
certain sort of rational dilemma. D&rsquo;Agostino and Sinigaglia
(2010) offer another argument for that epistemic utility function
based on the idea that accuracy is proximity to the ideal credence
function. Pettigrew (2016) also thinks of accuracy that way, but
offers an argument for additive and continuous strictly proper
epistemic utility functions, based on a suggestion by Frank P. Ramsey
(1926 [1931]) that connects accuracy and calibration, and Williams and
Pettigrew (2023) improve on that characterization. Based on a
pragmatic understanding of accuracy, where the utility of a credence
is the pragmatic utility it obtains for you through the practical
decisions it leads you to make, Levinstein (2017) builds on technical
results by Mark J. Schervish (1989) to characterize the additive and
continuous strictly proper scoring rules. And building on a suggestion
by Sophie Horowitz (2017) that the epistemic value of a credence
function is the quality of the educated guesses that it would license
you to make were you faced with a forced choice between guessing
various propositions, Gabrielle Kerbel (ms.) takes the accuracy of a
credence to be the average quality of the guesses your credence
licenses across a large range of forced choices it might be used to
make, and shows that this generates an additive and continuous
strictly proper epistemic utility function.</p>

<p>
The second sort of argument does not commit to a particular account of
epistemic value. Rather, it argues directly that, whatever is the
source of epistemic value, measures of it should have certain
properties. For instance, Joyce (2009) argues that measures of
epistemic value should be strictly proper as follows: If a measure of
epistemic utility is not strictly proper, then there is a
probabilistic credence function that doesn&rsquo;t expect itself to be
best, epistemically speaking. Because of this, that credence function
cannot be the unique rational response to any evidence, because it
thinks of an alternative as equally good, and so someone with that
credence function could rationally move to the alternative. But, for
any probabilistic credence function, there is a body of evidence to
which it is the unique rational response. So we have a contradiction.
Therefore, every legitimate measure of epistemic utility is strictly
proper. H&aacute;jek (2008) criticizes this argument and Pettigrew
(2016) defends Joyce.</p>

<p>
We now move to some general objections to these characterizations of
epistemic utility functions.</p>

<h4><a id="LevImp">5.1.1 The Varying Importance Objection</a> </h4>

<p>
According to Additivity, which is assumed by many of the accounts of
epistemic utility, the epistemic utility of a whole credence function
is the sum of the epistemic utilities of the individual credences it
comprises; and which scoring rule measures the epistemic utility of a
credence in a proposition at a world can depend on the proposition,
but not on the world. This allows the veritist, for instance, to
accommodate the fact that the accuracy of some propositions is more
important to us than the accuracy of others: I might take the accuracy
of a proposition about how many blades of grass there are on my lawn
to be less important than a proposition about the fundamental
constants of the universe. In such a case, I can simply take the
accuracy of a credence function to be a weighted sum of the epistemic
utilities of the individual credences, with greater weight given to
the propositions whose accuracy is more important to me. But Ben
Levinstein (2018) argues that the importance of a proposition does
sometimes depend on which world we inhabit: in worlds where I meet a
particular person and fall in love with them, propositions that
concern their well-being have great importance to me, and the
epistemic utility of my credences in those propositions should
contribute greatly to the epistemic utility of my whole credence
function; in worlds where I never meet that person, on the other hand,
the importance of these propositions is much diminished, as is the
contribution of the epistemic utility of my credences in them to my
total epistemic utility. And so the weights we assign to the scores of
the individual credences must also depend on the worlds; something
that Additivity rules out. And Levinstein shows further that, if we do
allow the weights to vary with the worlds, the resulting measure of
epistemic utility is no longer strictly proper, and indeed the
argument from
 <a href="#dom">Dominance</a>
 to
 <a href="#Pro">Probabilism</a>
 that we&rsquo;ll discuss in
 <a href="#EpiUtiArgPro">Section 5.2</a>
 fails if we use it.</p>

<h4><a id="OddVer">5.1.2 The Verisimilitude Objection</a> </h4>

<p>
Graham Oddie (2019) has argued that there is a source of epistemic
value that cannot be captured by any of the epistemic utility
functions that satisfy the conditions described above; this is the
virtue of <em>verisimilitude</em> (see entry on
 <a href="../truthlikeness/">truthlikeness</a>).
 His point is most easily introduced by an example. Suppose I am
interested in how many stars there are on the flag of Suriname. I have
credences in three propositions: <em>One</em>, <em>Two</em>, and
<em>Three</em>. In fact, there is exactly one star on the flag, so
<em>One</em> is true at the actual world, while <em>Two</em> and
<em>Three</em> are false. Now consider two different credence
functions on these three propositions:</p> 
\[
\begin{array}{r|ccc}
&amp; \textit{One} &amp; \textit{Two} &amp; \textit{Three} \\
\hline
C &amp; 0 &amp; 0.5 &amp; 0.5 \\
C' &amp; 0 &amp; 1 &amp; 0 
\end{array}
\]

<p>
So, \(C\) and \(C'\) both assign credence 0 to the true proposition,
<em>One</em>; they are certain that there are either two or three
stars on the flag, but while \(C\) spreads its credence equally over
these two false options, \(C'\) is certain of the first. According to
Oddie, \(C'\) has greater truthlikeness than \(C\) at the actual world
because it assigns a higher credence to a proposition that, while
false, is more truthlike, namely, <em>Two</em>, and it assigns a lower
credence to a proposition that is, while also false, less truthlike,
namely, <em>Three</em>. On this basis, he argues that any measure of
epistemic disutility must judge \(C\) to be worse than \(C'\).
However, he notes, nearly all measures of gradational accuracy
endorsed in epistemic utility theory will not judge in that way: they
will judge \(C'\) worse than \(C\). And indeed those that do so judge
will fail to respect truthlikeness in other ways. Jeffrey Dunn (2018)
and Miriam Schoenfield (2019) respond to Oddie&rsquo;s arguments.</p>

<h4><a id="MayWheNum">5.1.3 The Numerical Representability Objection</a> </h4>

<p>
We have considered a number of different characterizations of the
legitimate ways of measuring epistemic utility. Each has assumed that
the measures of these quantities are numerically representable; that
is, each assumes it makes sense to use real numbers to measure these
quantities. Conor Mayo-Wilson and Greg Wheeler call this assumption
into question (Mayo-Wilson &amp; Wheeler, ms.). They argue that, in
order to represent a quantity numerically, you need to prove a
representation theorem for it in measurement theory. And, if you wish
to use that quantity as a measure of utility, or as a component of a
measure of utility, you need to prove a representation theorem not
only for the quantity itself, but for its use in expected utility
calculations. They note that this was the purpose of the
representation theorems of von Neumann &amp; Morgenstern as well as
Savage and Jeffrey (see entry on
 <a href="../rationality-normative-utility/">normative theories of rational choice: expected utility</a>).
 And they argue that the methods that these authors use are not
available to the proponent of epistemic utility arguments. </p>

<h3><a id="EpiUtiArgPro">5.2 Epistemic utility arguments for Probabilism</a></h3>

<p>
Following the structure of epistemic utility arguments described in
 <a href="#ForArgEpiUtiThe">Section 2</a>,
 the arguments for Probabilism have three components: an account of
epistemic utility, which specifies a range of legitimate measures of
that quantity; a decision-theoretic norm; and a mathematical theorem
that derives the epistemic norm from the decision-theoretic norm when
the options are credence functions and utility is epistemic
utility.</p>

<blockquote>

<p id="Pro">
<strong>Probabilism</strong> Rationality requires that your credence
function at any given time is probabilistic.</p>
</blockquote>

<p>
Probabilism is one of a handful of norms that characterize the
Bayesian view in credal epistemology. The epistemic utility arguments
in its favor appeal to a slight weakening of the Dominance norm to
which we appealed above. We say that one option <em>strongly
dominates</em> another if it is better at all possible states of the
world, and we say it <em>weakly dominates</em> if it is at least as
good at all states of the world and better at some.</p>

<blockquote>

<p id="undominated-dominance">
<strong>Undominated Dominance</strong> If one option is strongly
dominated by another that isn&rsquo;t itself even weakly dominated,
then rationality requires that you should not adopt the first.</p>
</blockquote>

<p>
As before, we also appeal to a mathematical theorem to derive
Probabilism from the account of epistemic utility and the
decision-theoretic norm. The strongest theorem in the area is
this:</p>

<blockquote>

<p id="dom-thm-pro">
<strong>Dominance Theorem for Probabilism (de Finetti 1974; Savage
1971; Predd, et al. 2009; Pettigrew 2022; Nielsen 2022)</strong>
Suppose our epistemic utility function is continuous and strictly
proper. Then</p>

<ol type="i">

<li> Every non-probabilistic credence function defined on a particular
agenda is strongly dominated by a probabilistic credence function
defined on that agenda.</li>

<li> No probabilistic credence function defined on a particular agenda
is even weakly dominated by any credence function defined on that
agenda.</li>
</ol>
</blockquote>

<p>
See the
 <a href="maths-inacc.html#DomThmConHul">supplementary materials</a>
for a sketch of how the proof of (i) follows from the Dominance
Theorem for Convex Hulls.</p>
 
<p>
So, we have the following argument for Probabilism:</p>

<blockquote>

<p id="dom-arg-prob">
 <strong><a href="#continuity-eum">Continuity</a></strong>
 +
 <strong><a href="#strict-propriety-eum">Strict Propriety (for epistemic utility functions)</a></strong></p>
 
<p>
 <strong><a href="#undominated-dominance">Undominated Dominance</a></strong></p>
 
<p>
Therefore,</p>

<p>
 <strong><a href="#Pro">Probabilism</a></strong></p>
 </blockquote>

<p>
We now turn to objections to this argument.</p>

<h4><a id="BroObj">5.2.1 The Different Dominators Objection</a></h4>

<p>
Many of the existing characterizations of the legitimate epistemic
utility functions characterize a family of such measures; they do not
narrow the field to a single epistemic utility function. But, for all
that the
 <a href="#dom-thm-pro">Dominance Theorem for Probabilism</a>
 tells us, it may well be that, for a given non-probabilistic credence
function, different epistemic utility functions in such a family give
different sets of credence functions that dominate it. Thus, an agent
with a non-probabilistic credence function might be faced with a range
of alternative credence functions, each of which dominates theirs
relative to a different legitimate epistemic utility function.
Moreover, it may be that any credence function that dominates their
credence function relative to one epistemic utility function does not
dominate it relative to another; indeed, it may be that any credence
function that dominates theirs relative to the first risks very high
epistemic disutility at some world relative to the second, and
<em>vice versa</em>. In this situation, it is plausible that the agent
is rationally permitted to stick with her non-probabilistic credence
function. This objection was originally raised by Aaron Bronfman in
unpublished work, and it has been discussed by H&aacute;jek (2008) and
Pettigrew (2010, 2013b)</p>

<h4><a id="EviAcc">5.2.2 Evidence and Accuracy</a></h4>

<p>
According to
 <a href="#undominated-dominance">Undominated Dominance</a>,
 a dominated option is only ruled irrational in virtue of being
dominated if at least one of the options that dominate it is not
itself dominated. But there may be other features that a credence
function might have besides itself being dominated such that being
dominated by that credence function does not entail irrationality.
Kenny Easwaran and Branden Fitelson (2012) suggest such a feature.
Suppose that your credence function is non-probabilistic, but it
matches the evidence that you have: that is, the credence it assigns
to a proposition matches the extent to which your evidence supports
that proposition. And suppose that none of the credence functions that
dominate your credence function have that feature. Then, we might say,
the fact that your credence function is dominated does not rule it
irrational. For instance, suppose that a trick coin is about to be
tossed. Your evidence tells you that the chance of it landing heads is
0.7. Your credence that it will lands heads is 0.7 and your credence
that it will land tails is 0.6. Then you might think that your
credences match your evidence, because you have evidence only about it
landing heads and your credence that it will land heads equals the
known chance that it will land heads. However, it turns out that all
of the credence functions that dominate your credence function fail to
match this evidence, when epistemic utility is measured by the
 <a href="#Brier">Brier score</a>:
 that is, they assign credence other than 0.7 to the coin landing
heads. Pettigrew (2014a) and Joyce (2018) respond to this objection on
behalf of the dominance argument for Probabilism.</p>

<h4><a id="DomActStaDep">5.2.3 Dominance and Act-State Dependence</a></h4>

<p>
The final objection to the argument begins with the following sort of
case (Greaves 2013; Caie 2013; Campbell-Moore 2015):</p>

<blockquote>

<p>
<strong>Thwarted Accuracy</strong> Suppose I can read your mind. You
have opinions only about two propositions, \(X\) and \(\neg X\). And
suppose that I have control over the truth of \(X\) and \(\neg X\). I
decide to do the following. First, define the non-probabilistic
credence function \(C^\dag(X) = 0.8\) and \(C^\dag(\neg X) = 0.1\).
Then:</p>

<ol type="i">

<li>If your credence function is \(C^\dag\), I will make \(X\) true
(and thereby make your credence function very accurate);</li>

<li>If your credence function is not \(C^\dag\) and your credence in
\(X\) is greater than 0.5, I will make \(X\) false (and thereby make
your credence function rather inaccurate);</li>

<li>If your credence function is not \(C^\dag\) and your credence in
\(X\) is at most 0.5, I will make \(X\) true (and thereby make your
credence function rather inaccurate).</li>
</ol>
</blockquote>

<p>
Now \(C^\dag\) is not probabilistic, so there are credence functions
that are more accurate than \(C^\dag\) whether \(X\) is true or false.
However, because of the way I will manipulate the world in response to
your credences about it, if you adopt anything other than \(C^\dag\),
you&rsquo;ll end up less accurate. In such a case, it seems
rationality doesn&rsquo;t require us to have probabilistic credences.
The culprit here is
 <a href="#undominated-dominance">Undominated Dominance</a>.
 It is only plausible in cases in which the options between which the
agent is choosing will not influence the way the world is if they are
adopted. Such situations are sometimes called situations of
<em>act-state independence</em>.</p>

<p>
There are three responses available here: the first is to bite the
bullet, accept the restriction to
 <a href="#undominated-dominance">Undominated Dominance</a>,
 and therefore accept a restriction on the cases in which Probabilism
holds; the second is to argue that the practical case and the
epistemic case are different, with different decision-theoretic
principles applying to each; the third, of course, is to abandon the
accuracy argument for Probabilism. Joyce (2018) and Pettigrew (2018a)
argue for the first response. They advocate different
decision-theoretic principles to replace
 <a href="#undominated-dominance">Undominated Dominance</a>
 in the epistemic case: Joyce advocates standard causal decision
theory together with a Ratifiability condition (Jeffrey 1983);
Pettigrew omits the ratifiability condition. But they both agree that
these principles will agree with
 <a href="#undominated-dominance">Undominated Dominance</a>
 in cases of act-state independence; and they agree with the verdict
that \(C^\dag\) is the only credence function that isn&rsquo;t ruled
out as irrational in Thwarted Accuracy. Konek and Levinstein (2019)
argue for the second response, claiming that, since doxastic states
and actions have different directions of fit, different
decision-theoretic principles will govern them; and Kurt Sylvan (2020)
can be read as arguing for the same conclusion on the basis of his
claim that, while accuracy is the fundamental source of value for
epistemic states like credences, it is a value to which the
appropriate response is <em>respect</em>, not <em>promotion</em>. They
hold that
 <a href="#undominated-dominance">Undominated Dominance</a>
 is the correct principle when the options are credence functions,
even though it is not the correct principle when the options are
actions. Caie (2013) and Berker (2013a,b), on the other hand, argue
for the third option.</p>

<h4><a id="EpExp">5.2.4 Epistemic expansions</a></h4>

<p>
The
 <a href="#dom-thm-pro">Dominance Theorem for Probabilism</a>
 states: for epistemic utility functions of a particular sort, every
non-probabilistic credence function defined on a particular agenda is
dominated by an alternative probabilistic credence function,
<em>defined on that same agenda</em>, that is itself not dominated by
a further alternative <em>defined again on the same agenda</em>. But
you might think that this is still not sufficient to establish
 <a href="#Pro">Probabilism</a>.
 After all, while the dominating credence function is not itself
dominated by an alternative credence function defined on the same
agenda, it might be dominated by an alternative credence function
defined on a different agenda. For instance, take the
non-probabilistic credence function \(C^*\) defined on \(\mathcal{F} =
\{X, \neg X\}\), where \(C^*(X) = 0.6 = C^*(\neg X)\). Relative to the
Brier score, it is dominated by \(C'(X) = 0.5 = C'(\neg X)\). But
\(C'\) is Brier dominated by \(C^\dag\) defined on \(\mathcal{F}^\dag
= \{X\}\), where \(C^\dag(X) = 0.5\).</p>

<p>
A natural reaction to this is to define the epistemic utility of a
credence function to be the average epistemic utility of the credences
it assigns, rather than the total epistemic utility. For instance,
just as the Brier score is the total quadratic score of the credences
it assigns, we can define the <em>average Brier score</em> of a
credence function to be the average quadratic score of the credences
it assigns. Now, relative to the average Brier score, \(C^*\) is
indeed dominated by \(C'\) and \(C'\) is not dominated by \(C^\dag\).
But \(C'\) is dominated by \(C^+\) defined on \(\mathcal{F}^+ =
\{\top\}\), where \(C^+(\top) = 1\). Jennifer Carr (2015) initiated
the investigation into how epistemic utility arguments for Probabilism
might work when we start to compare credence functions defined on
different agendas. She notes the analogy with population axiology in
ethics (see entry on
 <a href="../repugnant-conclusion/">the repugnant conclusion</a>).
 Pettigrew (2018b) takes this analogy further, proving an
impossibility result analogous to those prevalent in that part of
ethics, and Brian Talbot (2022) presses the objection based on this
problem further.</p>

<h4><a id="InfProSpa">5.2.5 Infinite probability spaces</a></h4>

<p>
In the final two parts of this section, we ask what happens to the
argument for
 <a href="#Pro">Probabilism</a>
 when (i) we allow for infinite agendas and (ii) we allow the logic of
the propositions in those agendas to be non-classical.</p>

<p>
We have assumed throughout that the set of propositions on which an
agent&rsquo;s credence function is defined is finite. What happens
when we lift this restriction? The first problem is that we need to
say how to measure the epistemic utility of a credence function
defined over an infinite set of propositions. Then, having done that,
we need to say which such credence functions are dominated relative to
these measures, and which aren&rsquo;t.</p>

<p>
Sean Walsh has described an extension of the Brier score to the case
in which the set of propositions to which we assign credences is
countably infinite; and he has shown that non-probabilistic credence
functions on such sets are dominated relative to that measure, while
probabilistic ones are not. (For a description of Walsh&rsquo;s
unpublished work, see Kelley 2019). Mikayla Kelley (2019) has then
gone considerably further and generalized Walsh&rsquo;s results
significantly by describing a wide range of possible epistemic utility
functions and characterizing the undominated credence functions
defined on sets of propositions of different varieties; and Michael
Nielsen (2023) has generalized them in a different direction.</p>

<h4><a id="NonClaLog">5.2.6 Non-classical logic</a></h4>

<p>
In all of the arguments we&rsquo;ve surveyed above, we have assumed
that classical logic governs the propositions to which our agent
assigns credences. This secures
 <a href="#Pro">Probabilism</a>,
 which demands, among other things, that an agent assign maximal
credence to every classical tautology. But what happens if we drop
this assumption? What if, instead, the propositions are governed by a
three-valued logic, such as strong Kleene logic or the Logic of
Paradox (see entry on
 <a href="../logic-manyvalued/">many-valued logic</a>)?
 In a series of papers, Robbie Williams (2012a,b, 2018) has built on
mathematical results by Jeff Paris (2001) and Jean-Yves Jaffray (1989)
to understand what norms of credence the epistemic utility arguments
establish in this case. I&rsquo;ll give a single example here to
illustrate.</p>

<p>
Strong Kleene logic has three truth values: True, False, and Neither.
Our first question is this: what is the ideal credence in a
proposition that is neither true nor false? Williams argues that it
should be zero. And then he shows that, if the epistemic utility of a
credence at a world is its proximity to the ideal credence at that
world, and we measure the distance of one credence to another as the
square of the difference between them, as we do to generate the
quadratic scoring rule in the classical case, then the credence
functions that are not dominated are precisely those that satisfy the
norm of Generalized Probabilism:</p>

<blockquote>

<p>
<strong>Generalized Probabilism</strong> Suppose \(\models\) is the
logical consequence relation of the correct logic. Rationality
requires that your credence function \(C\) at a given time should be a
generalized probability function for that logic. That is:</p>

<ol type="i">

<li> If \(\bot \models\), then \(C(\bot) = 0\).</li>

<li> If \(\models \top\), then \(C(\top) = 1\).</li>

<li> If \(X \models Y\), then \(C(X)\leq C(Y)\).</li>

<li>\(C(X \vee Y) = C(X) + C(Y) - C(X \wedge Y)\).</li>
</ol>
</blockquote>

<p>
Note that, if \(\models\) is classical, then Generalized Probabilism
is equivalent to Probabilism.</p>

<p>
Williams (2018) also considers the case in which you are uncertain
which logic governs the propositions you consider, and Pettigrew
(2021) draws on a suggestion by Ian Hacking (1967) in a different
context to explore the case in which you know that the logic is
classical, but you don&rsquo;t know all the logical facts.</p>

<h3><a id="EpiUtiArgChCr">5.3 Epistemic utility arguments for chance-credence norms</a></h3>

<p>
In this section, we consider epistemic utility arguments for norms
that govern the relationship between the credences you assign to
propositions concerning objective chances and credences you assign to
propositions to which the objective chances assign probabilities: so,
for instance, the relationship between your credence in the
proposition that the chance of rain tomorrow is 76&percnt; and the
proposition that it will rain tomorrow (see entry on
 <a href="../chance-randomness/">chance and randomness</a>).
 The most well-known principle of this kind is the one that David
Lewis calls the Principal Principle (1980). To state it, we use the
following notation: if \(ch\) is a probability function, we write
\(\rho_{ch}\) for the proposition that says that \(ch\) gives the
objective chances. Then:</p>

<blockquote>

<p id="pp">
<strong>The Principal Principle</strong> Rationality requires that, if
\(C(\rho_{ch}) \gt 0\), and \(E\) is your total evidence, then \(C(X
\mid \rho_{ch}) = ch(X \mid E)\), for all \(X\) in your agenda. </p>
</blockquote>

<p>
So, for instance, your credence that it should rain tomorrow
conditional on the chance of rain tomorrow given all your evidence
being 76&percnt; should be 0.76.</p>

<p>
Now, as Lewis pointed out, this norm has implausible consequences if
the objective chance function might be <em>modest in the presence of
the body of evidence \(E\)</em>: that is, if the true chances might be
uncertain, given \(E\), that they give the true chances; that is, if
there is a probability function \(ch\) that might give the chances for
which \(ch(\rho_{ch} \mid E) \lt 1\). After all, by the Principal
Principle, if \(C(\rho_{ch}) \gt 0\), then \(C(\rho_{ch} \mid
\rho_{ch}) = ch(\rho_{ch} \mid E) \lt 1\), but by the definition of
conditional probability, \(C(\rho_{ch} \mid \rho_{ch}) = 1\).
Contradiction. So \(C(\rho_{ch}) = 0\). That is, if some of the
possible chance functions are modest in the presence of our evidence,
we must give them zero credence. And, as Lewis argued, the chances
posited by his account, which is known as Humeanism, will indeed be
modest in this sense, because they will give some positive probability
to the world being so different that its chances are also different
(Lewis 1980; see Section 3.6 of the entry on
 <a href="../probability-interpret/">Interpretations of Probability</a>).</p>
 
<p>
Nonetheless, if we assume that the chances are not modest, which is a
natural consequence of many non-Humean theories of chance, then we can
give an epistemic utility argument for the Principal Principle
(Pettigrew 2013, 2022). In the first premise, we assume Continuity and
Strict Propriety (for epistemic utility functions)&mdash;either
because credal veritism is true and the legitimate measures of
accuracy are continuous and strictly proper, or for other reasons. For
the second premise, we assume the following decision-theoretic norm,
where we say that one option <em>strongly chance dominates another in
the presence of \(E\)</em> if every possible chance function,
conditional on \(E\), gives higher expected utility to the first than
to the second, and we say that one option <em>weakly chance dominates
another in the presence of \(E\)</em> if every possible chance
function, conditional on \(E\), gives at least as high expected
utility to the first than to the second, and at least one possible
chance function, conditional on \(E\) gives strictly higher expected
utility to the first than to the second:</p>

<blockquote>

<p id="undom-ch-dom">
<strong>Undominated Chance Dominance</strong> If one option is
strongly chance dominated by an alternative in the presence of your
total evidence, and that alternative is not weakly chance dominated by
anything in the presence of your total evidence, then rationality
requires you not to choose the first. </p>
</blockquote>

<p>
And then we appeal to the following corollary of the
 <a href="#chance-dom-theorem">Chance Dominance Theorem for the General Chance-Credence Norm</a>,
 which we state below, to derive the
 <a href="#pp">Principal Principle</a>:</p>
 
<blockquote>

<p id="chance-dom-corollary">
<strong>Chance Dominance Corollary for the Principal
Principle</strong> Suppose your epistemic utility function is
continuous and strictly proper. And suppose no possible chance
function is modest in the presence of your evidence. Then:</p>

<ol type="i">

<li>If a credence function does not satisfy Probabilism + Principal
Principle, there is an alternative credence function that does satisfy
Probabilism + Principal Principle such that the latter strongly chance
dominates the former in the presence of your total evidence;</li>

<li>If a credence function does satisfy Probabilism + Principal
Principle, there is no alternative credence function that even weakly
chance dominates it in the presence of your total evidence.</li>
</ol>
</blockquote>

<p>
So, we have:</p>

<blockquote>

<p>
 <strong><a href="#continuity-eum">Continuity</a>
 +
 <a href="#strict-propriety-eum">Strict Propriety</a>
 (for epistemic utility functions)</strong></p>

<p>
 <strong><a href="#undom-ch-dom">Undominated Chance Dominance</a></strong></p>
 
<p>
Therefore,</p>

<p>
 <strong><a href="#Pro">Probabilism</a>
 +
 <a href="#pp">Principal Principle</a></strong></p>
 </blockquote>

<p>
Now, this argument does not fully justify the Principal Principle. A
full justification would also justify Undominated Chance Dominance by
explaining why chances are so special that rationality requires us to
reject an option when the possible chance functions unanimously reject
it. But what the justification does tell us is how we should respond
rationally to this deference we owe to the chances. After all, there
are alternative ways we might respond: we might say, for instance,
that your credence in \(X\) conditional on the chance of \(X\) being
greater than the chance of \(Y\) should be greater than your credence
in \(Y\) conditional on that same chance fact. So the argument does
give us something.</p>

<p>
So far, we&rsquo;ve assumed that chances are not modest. But in fact
we can still say something if they are. To state the next result, we
need to introduce a little terminology:</p>

<ul>

<li> a set of credence functions is <em>convex</em> if, whenever two
credences functions are in it, so is any
 <a href="#mixture">mixture</a>
 of them;</li>

<li> a set of credence functions is <em>closed</em> if, whenever there
is an infinite sequence of credence functions in it and they approach
arbitrarily close to another credence function in the limit, then the
credence function they approach is also in the set;</li>

<li> the <em>closed convex hull</em> of a set of credence functions is
the smallest closed and convex set that contains it; that is, for any
other set of credence functions that is closed and convex and contains
that set, the closed convex hull is a subset of it.</li>
</ul>

<blockquote>

<p id="chance-dom-theorem">
<strong>Chance Dominance Theorem for Chance-Credence Norms (Pettigrew
2022, Nielsen 2022)</strong> Suppose your epistemic utility function
is continuous and strictly proper. Then:</p>

<ol type="i">

<li>If your credence function does not lie in the closed convex hull
of the set of possible chance functions conditional on your evidence,
then there is an alternative credence function that does lie in that
closed convex hull such that the latter strongly chance dominates the
former;</li>

<li>If your credence function does lie in the closed convex hull of
the set of possible chance functions conditional on your evidence,
then there is no alternative credence function that weakly chance
dominates it.</li>
</ol>
</blockquote>

<p>
See the
 <a href="maths-inacc.html#DomThmConHul">supplementary materials</a>
for a sketch of how the proof of (i) follows from the Dominance
Theorem for Convex Hulls.</p>
 
<p>
Together with Continuity + Strict Propriety (for epistemic utility
functions) and Undominated Chance Dominance, this tells us that
rationality requires us to have a credence function that lies in the
closed convex hull of the possible chance functions. Now of course
that isn&rsquo;t a terribly intuitive condition. However, for various
properties weaker than immodesty that chance functions might all have,
this does entail that your credence function should also have that
property as well: all that is required is that, whenever two credence
functions have the property, any
 <a href="#mixture">mixture</a>
 of them does as well. Here are two such properties:</p>

<ul>

<li> \(C\) is <em>chance expectational in the presence of \(E\)</em>
if, for all \(X\) in \(\mathcal{F}\), 
\[
C(X) = \sum_{ch} C(\rho_{ch})ch(X \mid E)
\]
 </li>

<li> \(C\) <em>trusts the chances in the presence of \(E\)</em> if,
for all \(X\) in \(\mathcal{F}\), 
\[
C(X \mid ch(X \mid E) \geq x) \geq x
\]
 </li>
</ul>

<p>
Both are preserved under taking
 <a href="#mixture">mixtures</a>
 and so the Chance Dominance Theorem for Chance-Credence Norms gives
arguments for the following two chance-credence principles, if the
chances have the appropriate properties:</p>

<ul>

<li> <strong>General Recipe (Ismael 2008)</strong> Rationality
requires you to have a credence function that is chance expectational
in the presence of your total evidence.</li>

<li> <strong>Chance Trust Principle (Levinstein 2023)</strong>
Rationality requires you to trust the chances in the presence of your
total evidence.</li>
</ul>

<p>
However, Levinstein and Spencer (ms.) argue that the sort of Humean
account of chance that posits modest chance functions will also posit
chance functions that are neither chance-expectational nor trusting of
themselves as chances. So, for such accounts, these arguments for the
weaker chance-credence principles will not work.</p>

<p>
Levinstein (2023) offers a different epistemic utility argument for
the Chance Trust Principle. Undominated Chance Dominance is intended
to capture the claim that rationality requires us to defer to the
chances, and proposes a precise formulation of that demand. Levinstein
offers a different formulation. To defer to the chances epistemically
is to expect them to have greater expected epistemic utility than you
expect yourself to have (unless you are certain your credences match
the chances, in which case you expect them to have exactly as much
epistemic utility as you expect yourself to have). And, what&rsquo;s
more, you expect this regardless of the epistemic utility function you
use: provided it satisfies
 <a href="#additivity-eum">Additivity</a>,
 <a href="#continuity-eum">Continuity</a>, and
 <a href="#strict-propriety-eum">Strict Propriety</a>,
 if you&rsquo;re uncertain what the chances are, you should expect
them to do better than you expect your own credences to do. And,
Levinstein shows, if you do defer in this way, then you satisfy the
Chance Trust Principle.</p>

<h3><a id="EpiUtiArgCon">5.4 Epistemic utility arguments for Conditionalization</a></h3>

<p>
So far, we have been concerned with the so-called synchronic norms of
credence, that is, those that govern your credences at a particular
time. In this section, we turn to what are at least apparently
diachronic norms, that is, those that govern the relationship between
your credences at different times. The most well-known such norm is
Conditionalization, or Bayes&rsquo; Rule, which roughly tells you how
you should update your credences upon receiving some new evidence,
when that new evidence comes in the form of a proposition learned with
certainty.</p>

<blockquote>

<p id="dia-con">
<strong>Diachronic Conditionalization</strong> Suppose \(C\) is your
credence function at an earlier time \(t\) and \(C'\) is your credence
function at a later time \(t'\) and suppose that, between \(t\) and
\(t'\) the strongest proposition you learn is \(E\), then rationality
requires that, if \(C(E) \gt 0\), then for all \(X\) in
\(\mathcal{F}\), 
\[
C'(X) = C(X \mid E) = \frac{C(X\ \&amp; \ E)}{C(E)}
\]
 That is, at the later time, your
unconditional credence in a proposition should be equal to your
earlier credence in it conditional on the strongest proposition you
learned in the interim. If it is, we say that your posterior is
obtained from your prior by conditioning on your new evidence. </p>
</blockquote>

<p>
In fact, however, the original epistemic utility arguments in this
area did not attempt to establish Diachronic Conditionalization
directly. Rather, they attempted to establish that, when you know at
the earlier time that the evidence you&rsquo;ll receive will come from
a particular partition, then you should plan to update as Diachronic
Conditionalization demands. There are in fact two versions of the
resulting norm, depending on the scope of the rationality operator
(Greaves &amp; Wallace 2006; Briggs &amp; Pettigrew 2020).</p>

<blockquote>

<p id="pp-con-narrow">
<strong>Partitional Plan Conditionalization (narrow scope)</strong>
If</p>

<ul>

<li>\(C\) is your credence function at an earlier time \(t\),</li>

<li>the propositions \(E_1, \ldots, E_n\) form a partition,</li>

<li>between \(t\) and \(t'\), you&rsquo;ll learn which \(E_i\) is
true, and nothing more,</li>

<li>you plan to update as follows: if you learn \(E_i\), then
you&rsquo;ll adopt \(R_{E_i}\) as your credence function at
\(t'\),</li>
</ul>

<p>
then rationality requires that, if \(C(E_i) \gt 0\), then for all
\(X\) in \(\mathcal{F}\),</p> 
\[
 R_{E_i}(X) = C(X \mid E_i) = \frac{C(X\ \&amp; \ E_i)}{C(E_i)}
\]

</blockquote>

<p>
This says that, if \(C\) is your prior, rationality requires you to
plan to update upon receipt of new evidence by conditioning \(C\) on
that evidence.</p>

<blockquote>

<p id="pp-con-wide">
<strong>Partitional Plan Conditionalization (wide scope)</strong>
Rationality requires that: if</p>

<ul>

<li>\(C\) is your credence function at an earlier time \(t\),</li>

<li>the propositions \(E_1, \ldots, E_n\) form a partition,</li>

<li>between \(t\) and \(t'\), you&rsquo;ll learn which \(E_i\) is
true, and nothing more,</li>

<li> you plan to update as follows: if you learn \(E_i\), then
you&rsquo;ll adopt \(R_{E_i}\) as your credence function at
\(t'\),</li>
</ul>

<p>
then, if \(C(E_i) \gt 0\), then for all \(X\) in \(\mathcal{F}\),</p>

\[
 R_{E_i}(X) = C(X \mid E_i) = \frac{C(X\ \&amp; \ E_i)}{C(E_i)}
\]

</blockquote>

<p>
This says that rationality requires you not to have prior \(C\) and
yet plan to update upon receipt of new evidence in some way other than
by conditioning \(C\) on that new evidence.</p>

<h4><a id="EpiUtiArgPPC">5.4.1 Epistemic utility arguments for Partitional Plan Conditionalization</a></h4>

<p>
We begin with arguments for Partitional Plan Conditionalization.</p>

<p>
The first is due to Hilary Greaves and David Wallace (2008), building
on techniques from Peter M. Brown (1976) and Graham Oddie (1997). Some
terminology:</p>

<ul>

<li> An <em>updating plan</em> is a function from states of the world
to credence functions.</li>

<li> An updating plan is <em>available</em> if it takes the same
values at any two worlds at which your evidence will be the same.</li>

<li> An updating plan is a <em>conditionalizing plan for a prior
credence function</em> if, whenever you give positive credence to the
evidence you&rsquo;ll learn at a world, the plan tells you to update
at that world by conditioning the prior on that evidence.</li>
</ul>

<blockquote>

<p id="exp-thm-pp-con-n">
<strong>Expectation Theorem for Partitional Plan Conditionalization
(narrow scope) (Greaves &amp; Wallace 2008)</strong> Suppose your
epistemic utility function is strictly proper, and suppose your
evidence will tell you which member of a particular partition is true.
Then the updating plans that maximize expected epistemic utility from
the point of view of your prior credence function among those
available are exactly the conditionalizing plans for that prior.</p>
</blockquote>

<p>
I sketch the proof in the
 <a href="maths-inacc.html#ExpTheParPla">supplementary materials</a>.</p>
 
<p>
So, we have:</p>

<blockquote>

<p id="gre-wal">

 <a href="#strict-propriety-eum"><strong>Strict Propriety</strong></a>
 </p>

<p>
<strong>Maximize Expected Utility</strong></p>

<p>
Therefore,</p>

<p>

 <a href="#pp-con-narrow"><strong>Partitional Plan Conditionalization (narrow scope)</strong></a>
 </p>
</blockquote>

<p>
By asking about local updating plans, which say not which credence
function you plan to adopt upon receiving new evidence, but just what
credence you plan to assign to a particular proposition, Kenny
Easwaran (2013) extends this argument to establish a version of van
Fraassen&rsquo;s (1999) Reflection Principle and a norm known as
Conglomerability, which says that your unconditional credence in a
proposition should lie in the range spanned by your conditional
credences in it given different elements of a partition.</p>

<p>
Maximize Expected Utility is a well-known norm of decision theory, but
it is not universally accepted. Some decision theorists think that it
is rationally permissible to take risk into account in ways that
expected utility theory rules out. Building on a proposal by John
Quiggin (1982), Lara Buchak (2013) has provided a popular alternative
norm, Maximize Risk-Weighted Expected Utility, which allows you to
take risk into account. Catrin Campbell-Moore and Bernhard Salow
(2022) have explored an analogue of Greaves and Wallace&rsquo;s
argument that replaces Strict Propriety with its appropriately
risk-sensitive analogue and replaces Maximize Expected Utility with
Buchak&rsquo;s risk-sensitive version. They show that these do not
entail Partitional Plan Conditionalization, but rather an alternative
updating norm.</p>

<p>
The second argument for Partitional Plan Conditionalization is due to
Ray Briggs and Richard Pettigrew (2020), with an improvement by
Michael Nielsen (2021). They show that, assuming Additivity +
Continuity + Strict Propriety, if you look not only at the epistemic
utility of your updating plan, but at the sum of the epistemic utility
of your prior and the epistemic utility of your updating plan, then if
you violate Partitional Plan Conditionalization (wide scope), there is
an alternative prior and updating plan you might have had instead that
dominates yours.</p>

<blockquote>

<p id="dom-thm-pp-con-w">
<strong>Dominance Theorem for Partitional Plan Conditionalization
(wide scope) (Briggs &amp; Pettigrew 2020; Nielsen 2021)</strong>
Suppose your epistemic utility function is additive, continuous, and
strictly proper measure. Then:</p>

<ol>

<li> If your updating plan is available but it is not a
conditionalizing plan for your prior, then there is an alternative
prior and alternative available updating plan such that, at every
state of the world, the sum of the epistemic utility of your prior and
the epistemic utility of your updating plan is less than the sum of
the epistemic utility of the alternative prior and the epistemic
utility of the alternative updating plan.</li>

<li> If your updating plan is a conditionalizing plan for your prior,
then there is no alternative prior and alternative updating plan such
that, at every state of the world, the sum of the epistemic utility of
your prior and the epistemic utility of your updating plan is less
than the sum of the epistemic utility of the alternative prior and the
epistemic utility of the alternative updating plan.</li>
</ol>
</blockquote>

<p>
I sketch the proof in the
 <a href="maths-inacc.html#DomTheParPla">supplementary materials</a>.</p>
 
<p>
So, we have:</p>

<blockquote>

<p id="bri-pet">
 <strong><a href="#additivity-eum">Additivity</a>
 +
 <a href="#continuity-eum">Continuity</a>
 +
 <a href="#strict-propriety-eum">Strict Propriety</a>
 (for epistemic utility functions)</strong></p>

<p>
 <strong><a href="#undominated-dominance">Undominated Dominance</a></strong>
 </p>

<p>
Therefore,</p>

<p>
 <strong><a href="#pp-con-wide">Partitional Plan Conditionalization (wide scope)</a></strong></p>
 </blockquote>

<h4><a id="EpiUtiArgDC">5.4.2 Epistemic utility arguments for Diachronic Conditionalization</a></h4>

<p>
We turn now to two arguments for
 <a href="#dia-con">Diachronic Conditionalization</a>.
 Both take the same approach. You begin with your prior credence
function at the earlier time. Between the earlier and later time you
learn a proposition with certainty. Then, at the later time, you use
your prior credence function to decide what your new posterior
credence function should be. They differ in how they think that
decision should be made.</p>

<p>
According to Hannes Leitgeb and Richard Pettigrew (2010), you should
pick the posterior credence function that maximizes expected epistemic
utility from the point of view of your prior credence function, but
where the expectation is taken over only those worlds at which the
evidence is true. If your epistemic utility function is strictly
proper, and if your prior assigns positive credence to the proposition
you learn, then the unique posterior credence function that maximizes
this is the one demanded by
 <a href="#dia-con">Diachronic Conditionalization</a>.</p>
 
<p>
Campbell-Moore and Salow (2022) also consider an analogue of this
argument in the case of Buchak&rsquo;s risk-sensitive decision theory
and they show that, in this case, it does establish
 <a href="#dia-con">Diachronic Conditionalization</a>.</p>
 
<p>
According to Dmitri Gallow (2019), on the other hand, you should
maximize epistemic utility in the usual way, where the expectation is
taken over all worlds, but you should change the epistemic utility
function you use, so that it assigns the same neutral value to every
credence function at every world at which the evidence you&rsquo;ve
learned is false. Again it turns out that, if the epistemic utility
function you begin with is strictly proper, and if your prior assigns
positive credence to the proposition you learn, the credence function
that maximizes this is the one demanded by
 <a href="#dia-con">Diachronic Conditionalization</a>.</p>
 
<h4><a id="OthUpSit">5.4.3 Other updating situations</a></h4>

<p>
The updating norms we have considered so far and the arguments in
their favor make a number of assumptions. First, they assume that your
evidence will come in the form of a proposition learned with
certainty&mdash;we might call this assumption <em>Certainty</em>.
Second, they assume that proposition will be true&mdash;we might call
this assumption <em>Factivity</em>. Third, they assume that
proposition will come from a partition that can be specified in
advance&mdash;we might call this <em>Partitionality</em>. We'll treat
these three in turn.</p>

<p>
First, Certainty. Richard Jeffrey (1965) points out that our evidence
often doesn&rsquo;t come in the form of a proposition learned with
certainty, because there is often no proposition in our agenda that
perfectly captures what we learn. In such cases, he suggests, the
evidence places specific constraints on our posterior credences. He
considers the case in which it specifies which posterior credences you
must have in the propositions in a particular partition, and he
formulated a rule, known as Probability Kinematics or Jeffrey
Conditionalization, which tells you how to set your posterior
credences in other propositions that lie outside that partition.
Inspired by a suggestion by Diaconis and Zabell (1982), Leitgeb and
Pettigrew (2010) argue that, when your evidence places constraints on
your posterior credence function, you should update by adopting
whichever credence function maximizes expected epistemic utility from
the point of view of your prior credence function <em>among those
credence functions that satisfy the constraint</em>. Leitgeb and
Pettigrew show that the
 <a href="#Brier">Brier score</a>
 gives a different updating rule from the one that Jeffrey proposed.
Levinstein (2012) argues that this is a reason to reject the Brier
score, and Pettigrew (Theorem 12, Section 8.5, 2021) shows that no
additive strictly proper scoring rule gives Jeffrey&rsquo;s rule via
this approach.</p>

<p>
Jason Konek (2022) offers an alternative approach to the situations
that Jeffrey identified. He notes that, while there might be no
proposition in the agenda of your prior credence function that you
come to learn with certainty, there is a proposition that expresses
your experience, and you become able to entertain it when you have the
learning experience, because you can simply point to the learning
experience and say &lsquo;I learned that&rsquo;. So, after the
learning experience, you can add this proposition to your agenda and
retrospectively set your conditional credences in having that learning
experience, given different ways the world might be; and that is
sufficient to allow you to set your new credences given that you did
have that learning experience. He provides an epistemic utility
argument for a particular way of doing this.</p>

<p>
Second, Factivity. There are two ways to approach this, but the second
is also the way to approach Partitionality, so we&rsquo;ll leave that
for the moment. The first approach is suggested by Michael Rescorla
(2022), who asks not what you should plan to do when you learn which
proposition from a particular partition is true, but what you should
plan to do when you become certain of a proposition in a partition,
leaving open whether the proposition of which you&rsquo;ll become
certain will be true. Pettigrew (2023) shows that you should plan to
update by conditioning on what you learned with certainty by giving a
dominance argument for the following norm, due to Bas van Fraassen
(1999) (see also (Staffel &amp; de Bona forthcoming)):</p>

<blockquote>

<p id="WGRP">
<strong>Weak General Reflection Principle</strong> Rationality
requires that your prior credence function should be a
 <a href="#mixture">mixture</a>
 of your possible future credence functions. </p>
</blockquote> Pettigrew proves the following theorem:

<blockquote>

<p id="dom-thm-WGRP">
<strong>Dominance Theorem for Weak General Reflection (Pettigrew
2023)</strong> Suppose your epistemic utility function is additive,
continuous, and strictly proper measure. Then:</p>

<ol type="i">

<li> If your prior credence function is not a
 <a href="#mixture">mixture</a>
 of your possible posterior credence functions, then there is an
alternative prior and, for each possible posterior credence function,
an alternative posterior such that, at every state of the world and
for every possible posterior, the sum of the epistemic utility of your
prior and the epistemic utility of that posterior is less than the sum
of the epistemic utility of the alternative prior and the epistemic
utility of the alternative posterior.</li>

<li> If your prior credence function is a
 <a href="#mixture">mixture</a>
 of your possible posterior credence functions, then there is no
alternative prior and, for each possible posterior credence function,
an alternative posterior such that, at every state of the world and
for every possible posterior, the sum of the epistemic utility of your
prior and the epistemic utility of that posterior is less than the sum
of the epistemic utility of the alternative prior and the epistemic
utility of the alternative posterior.</li>
</ol>
</blockquote>

<p>
As van Fraassen (1999) shows, if we assume that (i) for each element
of a given partition, there is a unique possible posterior that is
certain of it, and (ii) for each possible posterior, there is a unique
element of the partition of which it is certain, then the Weak General
Reflection Principle entails that each possible posterior is obtained
from your prior by conditioning on the relevant element of the
partition.</p>

<p>
Third, Partitionality. To represent situations in which your evidence
does come in the form of a proposition learned with certainty, but in
which we assume neither that the proposition is true nor that it comes
from a partition that can be specified in advance, we follow Nilanjan
Das (2023) in defining an <em>evidence function</em> to be a function
that takes a state of the world and returns the proposition
you&rsquo;ll learn with certainty at that state of the world. As
before, an updating plan takes a state of the world to the posterior
you plan to adopt when you learn the evidence you&rsquo;ll receive at
that state of the world; and, as before, an updating plan is available
if it gives the same recommendation for any two states of the world at
which you receive the same evidence. Then Miriam Schoenfield (2016)
shows that you maximize expected epistemic utility not by planning to
conditionalize on your evidence, but by planning to conditionalize on
the fact that you received that evidence. Gallow (2021) worries that
the plans Schoenfield considers available are not genuinely available
to individuals in situations in which their evidence fails to rule out
that their evidence is different from how it actually is, and he
suggests a framework in which to represent genuinely available plans
and describes the updating plan that maximizes expected epistemic
utility in that framework.</p>

<h4><a id="UpOthSit">5.4.4 Updating in social situations</a></h4>

<p>
So far, we have only considered an individual&rsquo;s credences and
their relationship to one another. But we also learn from others who
investigate the world and share the credences they come to have on
that basis. So our epistemic situation affects and is affected by the
epistemic situation of others. Igor Douven and Sylvia Wenmackers
(2017) have explored a situation like this. They ask whether epistemic
utility considerations make different demands for updating from those
they make in the individual case. They suppose that the members of a
group all begin with the same prior credence function. Each holds a
coin and all the coins have the same bias towards landing heads, but
they are uncertain what that bias is to begin with. Each individual
tosses their coin some number of times and updates their own credences
on the basis of what they observe; then they share their evidence with
some of the other members of the group; then they repeat this process
a number of times. After each iteration, we measure their epistemic
utility, and then we look at the expected average epistemic utility
across all members and all times in the process. Douven and Wenmackers
treat updating on the private evidence from your own coin tosses and
updating on public evidence from the credences of others differently.
They assume each member updates on the public evidence of
others&rsquo; credences by taking the average (arithmetic mean) of
their credences and those of the other members. And then they ask
which updating rule for the private evidence will lead to the greatest
expected average epistemic utility, and they use computer simulation
to show that it is not Bayesian conditionalization. Indeed, they show
that an updating rule introduced by van Fraassen (Chapter 6, 1989) to
give a crude model of inference to the best explanation outperforms
Bayesian conditionalization (though we don&rsquo;t know whether some
other rule outperforms that). However as Pettigrew (2021b) points out,
this only shows that, when you update on the evidence of your peers by
taking averages, rather than by conditionalizing on it, you should
compensate for the suboptimality of that choice by doing something
other than conditionalizing on your private evidence. But of course
the results we&rsquo;ve considered in this section say you
shouldn&rsquo;t do that. You should update on the evidence of your
peers and the evidence of your private coin tosses as you should
update on everything, namely, by conditionalizing. </p>

<h4><a id="EpInq">5.4.4 The epistemology of inquiry</a></h4>

<p>
As we saw above, when Greaves and Wallace argue for Partitional Plan
Conditionalization (narrow scope), they hold fixed the partition from
which our evidence will come and ask which updating plan maximizes
expected epistemic utility. However, we don&rsquo;t simply receive
evidence passively. We also go out and seek it, and in those cases we
have to pick from which partition we want our evidence to come: Should
the scientist perform this experiment or another? Should the detective
interview this suspect or that? Adapting an insight due to I. J. Good
(1967) to the epistemic setting, Wayne Myrvold (2012) and Alejandro
P&eacute;rez Carballo (2018) show that we can use epistemic utilities
to tell which evidence we should collect from the epistemic point of
view.</p>

<p>
Given a partition, we know that an updating plan that maximizes
expected epistemic utility is a conditionalizing plan. So let&rsquo;s
hold fixed that, for any given partition, we plan to condition on
whichever proposition in it we learn with certainty. And now suppose
there are two partitions, and we must choose which one to investigate;
that is, we must choose from which one our evidence will come, always
assuming that, whichever we choose, we&rsquo;ll respond to the
evidence we receive by conditioning on it. Then we can simply compare
the expected epistemic utility of conditioning on whichever element of
the first partition we learn and the expected epistemic utility of
conditioning on whichever element of the second partition we learn,
and then do whichever is greater. One result is that, if one partition
is a fine-graining of the other, so that each proposition in the
latter is a disjunction of propositions in the former, we should
always choose the finer-grained one.</p>

<p>
As well as providing epistemic norms for choosing between the
partitions we might investigate, epistemic utilities can also give
norms for when to investigate at all and when to consider an issue
settled for the time being. To obtain these norms, we simply treat the
option of conducting no further investigations as the case in which we
investigate the trivial partition that consists only of a tautology.
If our epistemic utility function is strictly proper, and if you
assign any probability to investigating a given partition changing
your credences, then investigating must have greater expected
epistemic utility than not investigating, giving a epistemic analogue
to Good&rsquo;s value of information theorem (Good 1967; Myrvold
2012). But of course such investigations are not cost-free, and so the
question will always arise whether the cost is worth it for the
expected epistemic gain.</p>

<p>
Campbell-Moore and Salow (2020) show that, for risk-sensitive
individuals, investigating isn&rsquo;t always demanded, even when
it&rsquo;s free. It has long been known that such individuals are
sometimes pragmatically required not to investigate; Campbell-Moore
and Salow show that they are also sometimes epistemically required not
to.</p>

<p>
So epistemic utilities give epistemic norms that govern our choices
between different inquiries and our choice whether to investigate at
all, at least when other aspects are equal. But of course, things are
rarely equal. There are also pragmatic reasons for investigating one
partition rather than other, and so some reasons for inquiry are
pragmatic: for instance, investigating one partition might have
greater expected epistemic utility than investigating another, but it
might be more costly, or it might not help as much to inform a
pressing decision we must soon make. But Myrvold&rsquo;s and
P&eacute;rez Carballo&rsquo;s approach does at least tell us the
epistemic value of an investigation at a state of the world, which can
then be weighted against its pragmatic value and perhaps also moral
value to give the all-things-considered verdict on what to do.
Furthermore, this approach shows that there can be purely epistemic
reasons for gathering evidence for a particular investigation,
answering a question that has been raised in the literature on the
epistemology of inquiry (e.g. Woodard &amp; Flores forthcoming). And
Filippo Vindrola and Vincenzo Crupi (forthcoming) have recently
applied the approach to the Wason Selection Task to vindicate
Wason&rsquo;s original contention that people tend to choose how to
investigate irrationally in that case.</p>

<h3><a id="EpiArgUniThe">5.5 Epistemic utility arguments for and against the Uniqueness Thesis</a></h3>

<p>
In this section, we consider epistemic utility arguments for and
against the
 <a href="#uniqueness-thesis">Uniqueness Thesis</a>
 about credences. Recall that this says that, for any body of evidence
and any agenda, there is a unique credence function on that agenda
that it is rational for an individual with that total evidence to
have. And recall that epistemic permissivism about credences is the
negation of this claim. There are three sorts of argument. The first
appeals to norms of decision theory that encode attitudes to risk to
answer questions about which prior credence functions are rationally
permissible. The second appeals to the notions of probabilistic
knowledge and epistemic luck. The third appeals to the value of
rationality.</p>

<h4><a id="RisEpiPerUniThe">5.5.1 Epistemic risk and the Uniqueness Thesis</a></h4>

<p>
Minimax (sometimes called Maximin) is the most well-known norm of
decision theory that encodes an attitude to risk (Wald 1945). This
says that you should pick an option that <em>min</em>imizes your
worst-case or <em>max</em>imal disutility (equivalently, you should
pick an option that <em>maxi</em>mizes your worst-case or
<em>min</em>imal utility). If we say that someone is more risk-averse
the more weight they give to worst-case scenarios in their
decision-making and the less they give to best-case scenarios, Minimax
is maximally risk-averse.</p>

<p>
Pettigrew (2016) proves that, if we assume your agenda is an algebra
and our epistemic utility function is extensional and strictly proper,
then Minimax entails the Principle of Indifference:</p>

<blockquote>

<p id="poi">
<strong>Principle of Indifference</strong> Rationality requires that
you assign the same credence to every possible state of the world.</p>
</blockquote>

<p>
Given an agenda, Probabilism and the Principle of Indifference pick
out a unique prior credence function, namely, the uniform credence
function, \(c^\dag\), where \(c^\dag(w) = \frac{1}{n}\), for all \(w\)
in \(\mathcal{W}\), and \(n\) is the number of worlds in
\(\mathcal{W}\).</p>

<p>
However, Minimax is often thought too extreme. It might be rationally
permissible to be risk-averse, but it is not rationally permissible to
place <em>all</em> of your weight on the worst-case scenario and pay
<em>no</em> attention to anything else: you would surely prefer an
option that pays &pound;1 if the number of stars is even and
&pound;1,000,000 if it&rsquo;s odd to an option that pays &pound;2
either way, and yet Minimax rules out the first.</p>

<p>
Alternative risk-sensitive norms of decision theory have been
proposed. Pettigrew considers the Hurwicz Criterion (Hurwicz 1952;
Pettigrew 2016) and formulates the Generalized Hurwicz Criterion
(Pettigrew 2022). In the former, you consider not only the worst-case
scenario but also the best, and you assign a weight to each to give
the Hurwicz score. He shows that, if we are permissive enough about
the weightings, we obtain permissivism about rational priors. In the
latter, we give weights to all scenarios, best, second-best, and so on
down to second-worst, worst; and that gives us the generalized Hurwicz
score. Again, if we are permissive enough about weightings, we obtain
an even broader permissivism about priors. </p>

<h4><a id="EpLucProKno">5.5.2 Epistemic luck, probabilistic knowledge, and the Uniqueness Thesis</a></h4>

<p>
Jason Konek (2017) argues for a norm that&rsquo;s slightly weaker than
the Uniqueness Thesis. He appeals not to best and worst cases but to
best and worst expectations by the lights of different possible chance
functions. He is not so interested in what they think of the prior you
pick, but what they think of the possible posteriors you might obtain
from the prior when you learn new information. He thinks you should
pick a prior so that the difference between the maximum expected
epistemic utility of your posterior and the minimum is minimized: he
calls this principle MaxSen.</p>

<p>
Why do this? Because, Konek argues, this gives you the best chance of
forming credences that constitute probabilistic knowledge of the sort
Sarah Moss (2018) has described. One putative necessary condition on
knowledge is that the success you have forming an accurate belief or
credence is due to your own ability, rather than to luck. Konek argues
that, if you pick the prior he recommends, whatever accuracy you
obtain after you learn the new evidence and update accordingly is due
to your own cognitive ability as much as possible and to luck as
little as possible. The relationship of Konek&rsquo;s MaxSen to the
 <a href="#uniqueness-thesis">Uniqueness Thesis</a>
 is a bit subtle. For someone with no evidence, a fixed agenda, and a
partition from which their future evidence will come, there is a
unique credence function it demands; but which it demands does depend
on the partition from which the future evidence will come, and the
 <a href="#uniqueness-thesis">Uniqueness Thesis</a>
 does not strictly permit that.</p>

<h4><a id="ProEpiPerUniThe">5.5.3 Propriety, epistemic permissivism, and the Uniqueness Thesis</a></h4>

<p>
Sophie Horowitz (2019) appeals to epistemic utility theory to raise
two problems for epistemic permissivism.</p>

<p>
First, take a
 <a href="#strict-propriety-eum">strictly proper</a>
 epistemic utility function. Then, Horowitz points out, whichever set
of prior credence functions the epistemic permissivist takes to be
rationally permissible, for many of those permissible credence
functions, there will be impermissible ones they expect to have
greater epistemic utility than many of the other permissible ones. For
instance, perhaps rationality doesn&rsquo;t require that your prior
credence in a proposition is a particular number, but it does require
that it lies within a certain range&mdash;say, between 0.3 and 0.6.
Then, if your credence is near one end of this range but still within
it (say, 0.31), then it is rationally permitted, but it will expect
something a little beyond that end of the range (say, 0.29), which is
thereby not rationally permitted, to have greater epistemic utility
than something within the range, and therefore rationally permitted,
but at the other end (say, 0.59).</p>

<p>
Second, Horowitz notes that, if our epistemic utility function is
strictly proper, then each permissible credence function expects
itself to be best. And this causes problems for what Horowitz calls
&ldquo;acknowledged permissive cases&rdquo;. These are cases in which
rationality is permissive, and moreover the individual knows this is
so. In such cases, they adopt their particular prior, and they know
that this prior is merely one among many rationally permissible
priors; but, because our epistemic utility function is strictly
proper, they expect it to have greater epistemic utility than any
alternative, including the alternatives they take to be rationally
permissible. So, Horowitz asks, in what sense do they really consider
those alternatives permissible? Horowitz considers a response by
Miriam Schoenfield (2014) and finds it wanting.</p>

<h3><a id="EpiUtiArgSocEpi">5.6 Epistemic utility arguments in social epistemology</a></h3>

<p>
So far, we have considered only norms that govern an
individual&rsquo;s credences. But many epistemological questions arise
when we consider groups of individuals and their interactions. Here
are two such questions for which we have epistemic utility arguments:
Does the group itself have a credence function, just as its individual
members do, and if so how does relate to the credence functions of the
members?
 (<a href="#EpiUtiGroCre">Section 5.6.1</a>)
 How do we maximize the total epistemic utility across the group, and
does this place constraints on the individuals?
 (<a href="#MaxTotEpiUt">Section 5.6.2</a>).</p>
 
<h4><a id="EpiUtiGroCre">5.6.1 The epistemic utility of group credences</a></h4>

<p>
In everyday talk, we often ascribe beliefs and credences to groups of
individuals as well as to their members: the Intergovernmental Panel
on Climate Change is 70&percnt; confident in such-and-such; the
parliamentary subcommittee believes so-and-so. And we often think that
the credences of the individuals at least partly determine the
credences of the group. So suppose I must come up with precise
credences to ascribe to a group of individuals. The literature on
probabilistic judgment aggregation or opinion pooling offers a range
of possibilities. For instance:</p>

<blockquote>

<p id="poi-thm">
<strong>Linear Pooling</strong> A group&rsquo;s credence function
should be a
 <a href="#mixture">mixture</a>
 of the members&rsquo; credence functions.</p>
</blockquote>

<p>
Sarah Moss (2011) offers an epistemic utility argument for Linear
Pooling. She says that the group&rsquo;s credence function should be
the one that maximizes the group&rsquo;s expected epistemic utility,
and she defines the group&rsquo;s expected epistemic utility to be a
weighted arithmetic mean of the members&rsquo; individual expected
epistemic utilities. And then she proves that, providing our epistemic
utility function is strictly proper, it is the mixture of the
members&rsquo; credence functions with these same weights that
maximizes this quantity.</p>

<p>
One concern about this argument is that it assumes something too close
to what it attempts to establish. It assumes that the members&rsquo;
individual <em>expectations</em> should be combined by weighted
arithmetic averaging and attempts to show that members&rsquo;
individual <em>credence functions</em> should be so combined.
Pettigrew (2017) offers a related argument that improves on
Moss&rsquo;s in some ways, though makes very slightly stronger
assumptions. The idea is that, whatever the group&rsquo;s credence
function is, there had better not be an alternative that every member
of the group expects to be better, epistemically speaking. He then
shows that, if your epistemic utility function is continuous and
strictly proper, and if the group&rsquo;s credence function is not a
mixture of the individuals&rsquo; credence functions, then there is
such an alternative. </p>

<h4><a id="MaxTotEpiUt">5.6.2 Maximizing total epistemic utility across the group's members</a></h4>

<p>
As well as asking about the epistemic utility of the group&rsquo;s
credences, we can also ask about the total accuracy of the
members&rsquo; credences. And this gives an argument for a
surprisingly strong norm (Kopec 2012):</p>

<blockquote>

<p>
<strong>Consensus</strong> Rationality requires that all members of a
group have the same credence function.</p>
</blockquote>

<p>
The argument is based on the fact that, if a group violates Consensus,
so that at least two members disagree about some of their credences,
then there is a single credence function such that, if all members of
the group were to have it, their total epistemic utility would be
greater for sure.</p>

<p>
We should take this argument with a pinch of salt. After all, one
thing that people do is to investigate the world and collect evidence
and then share their findings. They decide which evidence to collect
by consulting their credences and asking which will maximize expected
epistemic or pragmatic utility, as we saw in
 <a href="#UpOthSit">Section 5.3.4</a>.
 And so the individuals in a group that satisfies Consensus will
conduct the same sorts of investigations and acquire similar evidence,
while a group with more diverse credences will conduct more diverse
investigations and end up with more diverse evidence. And it might
well be that the benefits a group gets in the long run from collecting
diverse evidence might outweigh the disadvantage they get from lacking
consensus at the start (Zollman 2010).</p>

<h2><a id="ComCon">6. Comparative confidence</a></h2>

<p>
In the comparative confidence model, we represent an
individual&rsquo;s epistemic state by their <em>comparative confidence
ordering</em>, which is an ordering or binary relation \(\prec, \sim\)
on their agenda. For any two propositions, \(X\) and \(Y\) in their
agenda, we say \(X \prec Y\) if they are less confident in \(X\) than
in \(Y\) and we say \(X \sim Y\) if they are exactly as confident in
\(X\) as in \(Y\).</p>

<p>
Our first job is to say how to measure the epistemic utility of such
an ordering. Fitelson and McCarthy (2014) suggest the following. A
comparative confidence ordering is a set of pairwise comparisons.
Fitelson and McCarthy assume that it is complete, so that, for any
\(X\), \(Y\) in the individual&rsquo;s agenda, \(X \prec Y\) or \(X
\sim Y\) or \(Y \prec X\). So we first say how to score these
individual comparisons: </p> 
\[
\begin{array}{c|c||c|c}
X &amp; Y &amp; X \prec Y &amp; X \sim Y \\
\hline
T &amp; T &amp; \alpha &amp; 1 \\
T &amp; F &amp; \beta &amp; \gamma \\
F &amp; T &amp; 1 &amp; \gamma \\
F &amp; F &amp; \alpha &amp; 1
\end{array}
\]

<p>
The idea is that, if \(X \prec Y\) and \(X\) is false and \(Y\) is
true, then you&rsquo;ve ordered the propositions as the omniscient
agent would, so you get maximal epistemic utility, which we&rsquo;ll
take to be 1; and similarly if \(X \sim Y\) and \(X\) and \(Y\) have
the same truth value. Then there are three potential mistakes whose
score we need to determine, and we've marked those scores as \(\alpha,
\beta, \gamma\).</p>

<p>
Now, we could simply permit any different values for these, but
Fitelson and McCarthy give an argument that we should set: \(\alpha =
1\), \(\beta = 0\), \(\gamma = 1/2\) (or any positive linear
transformation of these). Only for these scores does Fitelson and
McCarthy&rsquo;s account deliver a strictly proper way of measuring
epistemic utility, in the following sense:</p>

<ol>

<li> If \(C\) is a probabilistic credence function and \(C(X) \lt
C(Y)\), then \(C\) expects \(X \prec Y\) to be better than \(X \sim
Y\) and to be better than \(Y \prec X\).</li>

<li> If \(C\) is a probabilistic credence function and \(C(X) =
C(Y)\), then \(C\) expects \(X \sim Y\) to be better than \(X \prec
Y\) and to be better than \(Y \prec X\).</li>
</ol>

<p>
So the only strictly proper measure of inaccuracy for comparative
confidence orderings is this (up to positive linear transformation):

\[
\begin{array}{c|c||c|c}
X &amp; Y &amp; X \prec Y &amp; X \sim Y \\
\hline
T &amp; T &amp; 1 &amp; 1 \\
T &amp; F &amp; 0 &amp; 1/2 \\
F &amp; T &amp; 1 &amp; 1/2 \\
F &amp; F &amp; 1 &amp; 1
\end{array}
\]
 </p>

<p>
We can now ask: for which norms can we provide epistemic utility
arguments using this measure of epistemic utility? Here is one, where
we say that one proposition is strictly stronger than another if the
first entails the second, but the second does not entail the first:
</p>

<blockquote>

<p>
<strong>Strict Quasi-Additivity</strong></p>

<ul>

<li> If \(X\) is strictly stronger than \(Y\), \(X\) and \(Z\) are
mutually exclusive, and \(Y\) and \(Z\) are mutually exclusive, then
\(X \vee Z \prec Y \vee Z\). </li>

<li> If \(X\) is equivalent to \(Y\), \(X\) and \(Z\) are mutually
exclusive, and \(Y\) and \(Z\) are mutually exclusive, then \(X \vee Z
\sim Y \vee Z\). </li>
</ul>
</blockquote>

<p>
From this, we can derive a number of further norms:</p>

<blockquote>

<p>
<strong>Non-Triviality</strong> \(\bot \prec \top\), if \(\bot\) is a
contradiction and \(\top\) is a tautology.</p>

<p>
<strong>Regularity</strong> \(\bot \prec X \prec \top\), if \(\bot\)
is a contradiction, \(\top\) is a tautology, and \(X\) is
contingent.</p>

<p>
<strong>Strict Monotonicity</strong></p>

<ul>

<li> If \(X\) is strictly stronger than \(Y\), then rationality
requires that \(X \prec Y\).</li>

<li> If \(X\) is equivalent to \(Y\), then rationality requires that
\(X \sim Y\).</li>
</ul>
</blockquote>

<p>
Now, we have already assumed that the ordering is complete. If we add
Transitivity to Completeness and Strict Quasi-Additivity, then it is
guaranteed that there is a particular way to represent this ordering:
there is a Dempster-Shafer belief function \(b\) on \(\mathcal{F}\)
such that \(b(X) \lt b(Y)\) iff \(X \prec Y\) and \(b(X) = b(Y)\) iff
\(X \sim Y\) (Wong, et al. 1991).</p>

<p>
Eric Raidl and Wolfgang Spohn (2020) offer a slightly different way of
measuring the accuracy of a comparative confidence ordering and offer
an argument for the norms of Spohn&rsquo;s ranking theory (Spohn
2012).</p>

<h2><a id="ImpCre">7. Imprecise credences</a></h2>

<p>
In the imprecise credence model, we typically represent an
individual&rsquo;s epistemic state not as a single credence function,
but as a set of credence functions, and we assume those credence
functions are probabilistic. There are various related
representations, some of which are more powerful, some less, such as
upper and lower previsions, sets of desirable gambles, and probability
filters. I&rsquo;ll focus here on sets of probability functions.</p>

<p>
The central result in this area is that there are no measures of
epistemic utility for imprecise credences that have the property
analogous to the property of strict propriety in the precise case.
There are three versions of these results, starting with a theorem due
to Seidenfeld, Schervish, and Kadane (2012), and including adaptations
by Miriam Schoenfield (2017) and Conor Mayo-Wilson and Gregory Wheeler
(2016). I'll present Schoenfield&rsquo;s, since it&rsquo;s the most
straightforward.</p>

<p>
Schoenfield considers the simplest sort of case, where our individual
only has opinions about a proposition and its negation. And she
assumes that any measure of epistemic utility for imprecise credences
has the following properties:</p>

<blockquote>

<p>
<strong>Extension</strong> When restricted to precise probabilities
over those two propositions, the measure of epistemic utility is
maximized at the omniscient credence function and it is
continuous.</p>

<p>
<strong>Boundedness</strong> Epistemic utilities are measured by real
numbers. </p>

<p>
<strong>Probabilistic Admissibility</strong> For any precise credal
state, there is no imprecise credal state that is at least as good as
that precise state at all worlds. </p>
</blockquote>

<p>
Then Schoenfield shows that, if Extension, Boundedness, and
Probabilistic Admissibility hold, then for any imprecise set, there is
a precise probabilistic credence function that has equal epistemic
utility at every world. This seems problematic, because it suggests
that it can never be rationally required to have imprecise credences,
and their proponents typically think it can be.</p>

<p>
Jason Konek (2019) has proposed a way to avoid this concern. He argues
that, in fact, we should not expect a single measure of epistemic
utility to work for every situation. The idea is that our measure of
epistemic utility for imprecise credences encodes not only our
attitude to accuracy, but also our attitude to epistemic risk, and for
each legitimate way of measuring epistemic utility, there is a family
of imprecise credences that are endorsed by it. So all that is really
required is that, for any coherent imprecise credences, there is a
legitimate way of measuring epistemic utility that endorses them: that
is, where they consider themselves best from that point of view.</p>

<h2><a id="FutDir">8. Future directions</a></h2>

<p>
We have seen that epistemic utility arguments provide a fruitful
approach to epistemic norms. We conclude in this section by
highlighting some possible avenues for future research, though this
list is by no means exhaustive:</p>

<ul>

<li>One attraction of the epistemic utility approach is that it does
not only provide us with a means by which to establish norms that we
have already formulated; it also allows provides a process by which to
formulate new norms governing different situations that we can then
use it justify. The recipe is straightforward: specify the situation;
specify your measure of epistemic utility; ask what properties an
epistemic state must have if it is to optimize that measure of
epistemic utility in that situation. For instance, it is this approach
that lead from Joyce&rsquo;s argument for Probabilism, to Greaves
&amp; Wallace&rsquo;s argument for Conditionalization, to
Myrvold&rsquo;s approach to the value of learning new evidence.
Increasingly, epistemologists are considering less and less idealized
epistemic situations: situations in which you have evidence, but you
don&rsquo;t know what that evidence is (Williamson 2000); situations
in which you have higher-order evidence that casts doubt on your
first-order reasoning (Feldman 2005); and so on. The epistemic utility
approach is ideally situated to help formulate norms to govern these
non-ideal situations.</li>

<li>While there is some work on how to choose between different ways
of measuring epistemic utility, more could be done (Pettigrew 2016,
Levinstein 2017). What reason do we have for using one strictly proper
scoring rule rather than another?</li>

<li>While much work on epistemic utility assumes, along with the
veritist, that the sole fundamental source of epistemic value is truth
or gradational accuracy, other parts of epistemology talk of further
or alternative sources, such as explanatory power or being formed by
an epistemically virtuous process. How would we measure epistemic
utility if these are among its sources?</li>

<li>A final, rather obvious suggestion is that there are substantial
objections to the approach that must still be addressed. We have met
many of them over the course of this survey, from Levinstein&rsquo;s
Varying Importance Objection in
 <a href="#LevImp">Section 5.1.1</a>
 to the Act-State Dependence worry in
 <a href="#DomActStaDep">Section 5.2.3</a>
 to Jennifer Carr&rsquo;s epistemic expansions concern in
 <a href="#EpExp">Section 5.2.4</a>.
 These three, as well as others detailed above, still lack truly
compelling responses.</li>
</ul>

<p>
There is still much to be done.</p>
</div>

<div id="bibliography">

<h2><a id="Bib">Bibliography</a></h2>

<ul class="hanging">

<li>Alchourr&oacute;n, C.E., P. G&auml;rdenfors, &amp; D. Makinson,
1985, &ldquo;On the Logic of Theory Change: Partial Meet Contraction
and Revision Functions&rdquo;, <em>Journal of Symbolic Logic</em>, 50:
510&ndash;530.</li>

<li>Berker, S., 2013a, &ldquo;Epistemic Teleology and the Separateness
of Propositions&rdquo;, <em>Philosophical Review</em>, 122(3):
337&ndash;393.</li>

<li>&ndash;&ndash;&ndash;, 2013b, &ldquo;The Rejection of Epistemic
Consequentialism&rdquo;, <em>Philosophical Issues (Supp.
No&ucirc;s)</em>, 23(1): 363&ndash;387.</li>

<li>BonJour, L., 1985, <em>The Structure of Empirical Knowledge</em>,
Cambridge, MA: Harvard University Press.</li>

<li>Briggs, R. A. &amp; R. Pettigrew, 2020, &ldquo;An
Accuracy-Dominance Argument for Conditionalization&rdquo;,
<em>No&ucirc;s</em>, 54(1): 162&ndash;181.</li>

<li>Brown, P. M., 1976, &ldquo;Conditionalization and expected
utility&rdquo; <em>Philosophy of Science</em>, 43(3):
415&ndash;419.</li>

<li>Buchak, L., 2013, <em>Risk and Rationality</em>, Oxford: Oxford
University Press.</li>

<li>Caie, M., 2013, &ldquo;Rational Probabilistic Incoherence&rdquo;,
<em>Philosophical Review</em>, 122(4): 527&ndash;575.</li>

<li>Campbell-Moore, C., 2015, &ldquo;Rational Probabilistic
Incoherence? A Reply to Michael Caie&rdquo;, <em>Philosophical
Review</em> 124(3): 393&ndash;406.</li>

<li>&ndash;&ndash;&ndash;, &amp; B. Salow 2020, &ldquo;Avoiding Risk
and Avoiding Evidence&rdquo;, <em>Australasian Journal of
Philosophy</em>, 98(3): 495&ndash;515.</li>

<li>&ndash;&ndash;&ndash;, &amp; B. Salow 2022, &ldquo;Accurate
Updating for the Risk&ndash;Sensitive&rdquo;, <em>The British Journal
for the Philosophy of Science</em>, 73(3): 751&ndash;776.</li>

<li>Carr, J., 2015, &ldquo;Epistemic Expansions&rdquo;, <em>Res
Philosophica</em>, 92(2): 217&ndash;236.</li>

<li>&ndash;&ndash;&ndash;, 2017, &ldquo;Epistemic Utility Theory and
the Aim of Belief&rdquo;, <em>Philosophy and Phenomenological
Research</em>, 95(3): 511&ndash;534.</li>

<li>&ndash;&ndash;&ndash;, 2019, &ldquo;A Modesty Proposal&rdquo;,
<em>Synthese</em>. doi:10.1007/s11229-019-02301-x</li>

<li>Cox, R. T., 1946, &ldquo;Probability, Frequency and Reasonable
Expectation&rdquo;, <em>American Journal of Physics</em>, 14:
1&ndash;13.</li>

<li>&ndash;&ndash;&ndash;, 1961, <em>The Algebra of Probable
Inference</em>, Baltimore: Johns Hopkins University Press.</li>

<li>D&rsquo;Agostino, M. &amp; C. Sinigaglia, 2010, &ldquo;Epistemic
Accuracy and Subjective Probability&rdquo;, in M. Dorato &amp; M.
Su&agrave;rez (eds.), <em>EPSA Epistemology and Methodology of
Science</em>, Springer.</li>

<li>Das, N., &ldquo;The Value of Biased Information&rdquo;, <em>The
British Journal for the Philosophy of Science</em>, 74(1):
25&ndash;55.</li>

<li>de Finetti, B., 1937 [1980], &ldquo;Foresight: Its Logical Laws,
Its Subjective Sources&rdquo;, in H. E. Kyburg &amp; H. E. K. Smokler
(eds.), <em>Studies in Subjective Probability</em>, Huntington, NY:
Robert E. Kreiger Publishing Co., 1980</li>

<li>&ndash;&ndash;&ndash;, 1974, <em>Theory of Probability</em> Vol.
1, New York: Wiley.</li>

<li>Diaconis, P. &amp; S. Zabell, 1982, &ldquo;Updating Subjective
Probability&rdquo; <em>Journal of the American Statistical
Association</em>, 77(380): 822&ndash;830.</li>

<li>Dorst, K., 2017, &ldquo;Lockeans Maximize Expected
Accuracy&rdquo;, <em>Mind</em>, 128(509): 175&ndash;211.</li>

<li>Douven, I. &amp; S. Wenmackers, 2017, &ldquo;Inference to the Best
Explanation versus Bayes???s Rule in a Social Setting&rdquo;,
<em>British Journal for the Philosophy of Science</em>, 68(2):
535&ndash;570.</li>

<li>Dunn, J., 2018, &ldquo;Accuracy, Verisimilitude, and Scoring
Rules&rdquo;, <em>Australasian Journal of Philosophy</em>, 97(1):
151&ndash;166.</li>

<li>Easwaran, K., 2013, &ldquo;Expected Accuracy Supports
Conditionalization&mdash;and Conglomerability and Reflection&rdquo;,
<em>Philosophy of Science</em>, 80(1): 119&ndash;142.</li>

<li>&ndash;&ndash;&ndash;, 2015, &ldquo;Accuracy, Coherence, and
Evidence&rdquo;, <em>Oxford Studies in Epistemology</em>, 5,
61&ndash;96.</li>

<li>&ndash;&ndash;&ndash;, 2016, &ldquo;Dr Truthlove, Or: How I
Learned to Stop Worrying and Love Bayesian Probabilities&rdquo;,
<em>No&ucirc;s</em>, 50(4): 816&ndash;853</li>

<li>&ndash;&ndash;&ndash; &amp; B. Fitelson, 2012, &ldquo;An
&lsquo;evidentialist&rsquo; worry about Joyce&rsquo;s argument for
Probabilism&rdquo;, <em>Dialectica</em>, 66(3): 425&ndash;433.</li>

<li>Feldman, R., 2005, &ldquo;Respecting the Evidence&rdquo;,
<em>Philosophical Perspectives</em>, 19(1): 95&ndash;119.</li>

<li>Fitelson, B. &amp; K. Easwaran, 2015, &ldquo;Accuracy, Coherence
and Evidence&rdquo;, <em>Oxford Studies in Epistemology</em>, 5:
61&ndash;96.</li>

<li>Fitelson B. &amp; D. McCarthy, 2014, &ldquo;Toward an epistemic
foundation for comparative confidence&rdquo;, Presentation, University
of Wisconsin-Madison.
 <a href="http://fitelson.org/cc_handout.pdf" target="other">[Online version available here]</a>.</li>
 
<li>Flores, C. &amp; E. Woodard, forthcoming, &ldquo;Epistemic Norms
on Evidence&ndash;Gathering&rdquo;, <em>Philosophical
Studies</em>.</li>

<li>Foley, R., 1992, &ldquo;The Epistemology of Belief and the
Epistemology of Degrees of Belief&rdquo;, <em>American Philosophical
Quarterly</em>, 29(2): 111&ndash;121.</li>

<li>Fraassen, B.C. van, 1983, &ldquo;Calibration: Frequency
Justification for Personal Probability&rdquo;, in R.S. Cohen &amp; L.
Laudan (eds.), <em>Physics, Philosophy, and Psychoanalysis</em>,
Dordrecht: Springer.</li>

<li>Gallow, J. D., 2019, &ldquo;Learning and Value Change&rdquo;,
<em>Philosophers&rsquo; Imprint</em>, 19: 1&ndash;22.</li>

<li>&ndash;&ndash;&ndash;, 2021, &ldquo;Updating for
Externalists&rdquo;, <em>No&ucirc;s</em>, 55(3): 487&ndash;516.</li>

<li>Goldman, A.I., 1999, <em>Knowledge in a Social World</em>, New
York: Oxford University Press.</li>

<li>Good, I. J., 1967, &ldquo;On the Principle of Total
Evidence&rdquo;, <em>The British Journal for the Philosophy of
Science</em>, 17(4):319&ndash;321.</li>

<li>Greaves, H., 2013, &ldquo;Epistemic Decision Theory&rdquo;,
<em>Mind</em>, 122(488): 915&ndash;952.</li>

<li>Greaves, H. &amp; D. Wallace, 2006, &ldquo;Justifying
Conditionalization: Conditionalization Maximizes Expected Epistemic
Utility&rdquo;, <em>Mind</em>, 115(459): 607&ndash;632.</li>

<li>Hacking, I., 1967, &ldquo;Slightly More Realistic Personal
Probability&rdquo;, <em>Philosophy of Science</em>, 34(4):
311&ndash;325.</li>

<li>Harman, G., 1973, <em>Thought</em>, Princeton, NJ: Princeton
University Press.</li>

<li>H&aacute;jek, A., 2008, &ldquo;Arguments For&mdash;Or
Against&mdash;Probabilism?&rdquo;, <em>The British Journal for the
Philosophy of Science</em>, 59(4): 793&ndash;819.</li>

<li>&ndash;&ndash;&ndash;, 2009, &ldquo;Fifteen Arguments against
Hypothetical Frequentism&rdquo;, <em>Erkenntnis</em>, 70:
211&ndash;235.</li>

<li>Hempel, C., 1962, &ldquo;Deductive&ndash;Nomological vs.
Statistical Explanation&rdquo;, in H. Feigl and &amp; G. Maxwell
(eds.) <em>Minnesota Studies in the Philosophy of Science (vol.
III)</em>, Minneapolis:University of Minnesota Press.</li>

<li>Horowitz, S., 2014, &ldquo;Immoderately rational&rdquo;,
<em>Philosophical Studies</em>, 167: 41&ndash;56.</li>

<li>&ndash;&ndash;&ndash;, 2017, &ldquo;Accuracy and Educated
Guesses&rdquo; in T. Szab&oacute; Gendler &amp; J. Hawthorne (eds.)
<em>Oxford Studies in Epistemology, volume 6</em> Oxford: Oxford
University Press.</li>

<li>&ndash;&ndash;&ndash;, 2019, &ldquo;The Truth Problem for
Permissivism&rdquo;, <em>Journal of Philosophy</em> 116(5):
237&ndash;262.</li>

<li>Hughes, R. I. G. &amp; B. C. van Fraassen, 1984, &ldquo;Symmetry
arguments in probability kinematics&rdquo;, in <em>PSA: Proceedings of
the Biennial Meeting of the Philosophy of Science Association</em>
(Issue 2/Volume Two: Symposia and Invited Papers), pp. 851&ndash;869
doi:10.1086/psaprocbienmeetp.1984.2.192543</li>

<li>Hurwicz, L., 1952, &ldquo;A criterion for decision making under
uncertainty&rdquo;, Cowles Commission Technical Report 355.</li>

<li>Huttegger, S.M., 2013, &ldquo;In Defense of Reflection&rdquo;,
<em>Philosophy of Science</em>, 80(3): 413&ndash;433.</li>

<li>Ismael, J., 2008, &ldquo;Raid! Dissolving the Big, Bad Bug&rdquo;,
<em>No&ucirc;s</em>, 42(2): 292&ndash;307.</li>

<li>Jaffray, J-Y., 1989, &ldquo;Coherent bets under partially
resolving uncertainty and belief functions&rdquo;, <em>Theory and
Decision</em>, 26: 90&ndash;105.</li>

<li>James, W., 1897, &ldquo;The Will to Believe&rsquo;, in <em>The
Will to Believe, and Other Essays in Popular Philosophy</em>, New
York: Longmans Green.</li>

<li>Jeffrey, R., 1965, <em>The Logic of Decision</em>, New York:
McGraw-Hill.</li>

<li>Jeffrey, R., 1983, <em>The Logic of Decision</em>
(2<sup>nd</sup>). Chicago; London: University of Chicago Press.</li>

<li>Jenkins, C.S., 2007, &ldquo;Entitlement and Rationality&rdquo;,
<em>Synthese</em>, 157: 25&ndash;45.</li>

<li>Joyce, J.M., 1998, &ldquo;A Nonpragmatic Vindication of
Probabilism&rdquo;, <em>Philosophy of Science</em>, 65(4):
575&ndash;603.</li>

<li>&ndash;&ndash;&ndash;, 2009, &ldquo;Accuracy and Coherence:
Prospects for an Alethic Epistemology of Partial Belief&rdquo;, in F.
Huber &amp; C. Schmidt-Petri (eds.), <em>Degrees of Belief</em>,
Springer.</li>

<li>&ndash;&ndash;&ndash;, 2018, &ldquo;The True Consequences of
Epistemic Consequentialism&rdquo;, in Ahlstrom-Vij &amp; Dunn
2018.</li>

<li>Kelley, M., 2019, <em>Accuracy Dominance on Infinite Opinion
Sets</em>, MA Thesis, UC Berkeley.
 <a href="https://mikaylalynnkelley.weebly.com/uploads/1/1/7/0/117045875/ma_thesis_final_draft.pdf" target="other">[Online version available here]</a>.</li>
 
<li>Kelly, T., 2014, &ldquo;Evidence Can Be Permissive&rdquo;, in M.
Steup, J. Turri, &amp; E. Sosa (eds.) <em>Contemporary Debates in
Epistemology</em>, Oxford: Wiley-Blackwell.</li>

<li>Kerbel, G., ms, &ldquo;A New Approach to Scoring on the Educated
Guessing Framework&rdquo;, Unpublished manuscript.</li>

<li>Konek, J., 2016, &ldquo;Probabilistic Knowledge and Cognitive
Ability&rdquo;, <em>Philosophical Review</em>, 125(4):
509&ndash;587.</li>

<li>&ndash;&ndash;&ndash;, 2019, &ldquo;IP Scoring Rules: Foundations
and Applications&rdquo;, <em>Proceedings of Machine Learning
Research</em>, 103: 256&ndash;264.</li>

<li>&ndash;&ndash;&ndash;, 2022, &ldquo;The Art of Learning&rdquo; in
T. Szab&oacute; Gendler &amp; J. Hawthorne (eds.) <em>Oxford Studies
in Epistemology, volume 7</em> Oxford: Oxford University Press.</li>

<li>Konek, J. &amp; B.A. Levinstein, 2019, &ldquo;The Foundations of
Epistemic Decision Theory&rdquo;, <em>Mind</em>, 128(509):
69&ndash;107.</li>

<li>Kopec, M., 2012, &ldquo;We Ought to Agree: A Consequence of
Repairing Goldman&rsquo;s Group Scoring Rule&rdquo;,
<em>Episteme</em>, 9: 101&ndash;14.</li>

<li>Leitgeb, H., 2021, &ldquo;A Structural Justification of
Probabilism: From Partition Invariance to Subjective
Probability&rdquo;, <em>Philosophy of Science</em>, 88(2):
341&ndash;365.</li>

<li>Leitgeb, H. &amp; R. Pettigrew, 2010a, &ldquo;An Objective
Justification of Bayesianism I: Measuring Inaccuracy&rdquo;,
<em>Philosophy of Science</em>, 77: 201&ndash;235.</li>

<li>&ndash;&ndash;&ndash;, 2010b, &ldquo;An Objective Justification of
Bayesianism II: The Consequences of Minimizing Inaccuracy&rdquo;,
<em>Philosophy of Science</em>, 77: 236&ndash;272.</li>

<li>Levinstein, B. A., 2012, &ldquo;Leitgeb and Pettigrew on Accuracy
and Updating&rdquo;, <em>Philosophy of Science</em>, 79(3):
413&ndash;424.</li>

<li>&ndash;&ndash;&ndash;, 2015, &ldquo;With All Due Respect: The
Macro-Epistemology of Disagreement&rdquo;, <em>Philosophers&rsquo;
Imprint</em>, 15(3): 1&ndash;20.</li>

<li>&ndash;&ndash;&ndash;, 2017, &ldquo;A Pragmatist&rsquo;s Guide to
Epistemic Utility&rdquo;, <em>Philosophy of Science</em> 84(4):
613&ndash;638.</li>

<li>&ndash;&ndash;&ndash;, 2018, &ldquo;An Objection of Varying
Importance to Epistemic Utility Theory&rdquo;, <em>Philosophical
Studies</em>. doi:10.1007/s11098-018-1157-9</li>

<li>&ndash;&ndash;&ndash;, 2023, &ldquo;Accuracy, Deference, and
Chance&rdquo; <em>Philosophical Review</em> 132(1): 43&ndash;87.</li>

<li>Lewis, D., 1980, &ldquo;A Subjectivist&rsquo;s Guide to Objective
Chance&rdquo;, in R.C. Jeffrey (ed.), <em>Studies in Inductive Logic
and Probability</em> (Vol. II). Berkeley: University of California
Press.</li>

<li>Locke, J., 1689 [1975] <em>An Essay Concerning Human
Understanding</em>, P.H. Nidditch (ed.) Oxford: Clarendon Press.</li>

<li>Maher, P., 1993, <em>Betting on Theories</em>, Cambridge:
Cambridge University Press.</li>

<li>&ndash;&ndash;&ndash;, 2002, &ldquo;Joyce&rsquo;s Argument for
Probabilism&rdquo;, <em>Philosophy of Science</em>, 69(1):
73&ndash;81.</li>

<li>Mayo-Wilson, C. &amp; G. Wheeler, 2016, &ldquo;Scoring Imprecise
Credences: A Mildly Immodest Proposal&rdquo;, <em>Philosophy and
Phenomenological Research</em>, 92(1): 55&ndash;78.</li>

<li>&ndash;&ndash;&ndash;, ms., &ldquo;Epistemic Decision
Theory&rsquo;s Reckoning&rdquo;. Unpublished manuscript.
 <a href="http://philsci-archive.pitt.edu/16374/" target="other">[Online version available here]</a>.
 </li>

<li>Meacham, C. J. G., 2018, &ldquo;Can All-Accuracy Accounts Justify
Evidential Norms&rdquo;, in Ahlstrom-Vij &amp; Dunn 2018.</li>

<li>Moss, S., 2011, &ldquo;Scoring Rules and Epistemic
Compromise&rdquo;, <em>Mind</em>, 120(480): 1053&ndash;1069.</li>

<li>&ndash;&ndash;&ndash;, 2018, &ldquo;Probabilistic
Knowledge&rdquo;, Oxford: Oxford University Press.</li>

<li>Myrvold, W., 2012, &ldquo;Epistemic values and the value of
learning&rdquo;, <em>Synthese</em>, 187: 547&ndash;568.</li>

<li>Nielsen, M., 2021, &ldquo;Accuracy-dominance and
conditionalization&rdquo;, <em>Philosophical Studies</em>, 178(10):
3217&ndash;3236.</li>

<li>&ndash;&ndash;&ndash;, 2022, &ldquo;On the Best Accuracy Arguments
for Probabilism&rdquo;, <em>Philosophy of Science</em>, 89(3):
621&ndash;630.</li>

<li>&ndash;&ndash;&ndash;, 2023, &ldquo;Accuracy and Probabilism in
Infinite Domains&rdquo; <em>Mind</em>, 132(526): 402&ndash;427.</li>

<li>Oddie, G., 1997, &ldquo;Conditionalization, cogency, and cognitive
value&rdquo;, <em>British Journal for the Philosophy of Science</em>,
48(4): 533&ndash;541.</li>

<li>&ndash;&ndash;&ndash;, 2019, &ldquo;What Accuracy Could Not
Be&rdquo;, <em>The British Journal for the Philosophy of Science</em>,
70(2): 551&ndash;580.</li>

<li>Paris, J. B., 1994, <em>The Uncertain Reasoner&rsquo;s Companion:
A Mathematical Perspective</em>, Cambridge: Cambridge University
Press.</li>

<li>&ndash;&ndash;&ndash;, 2001, &ldquo;A Note on the Dutch Book
Method&rdquo;, <em>Proceedings of the Second International Symposium
on Imprecise Probabilities and their Applications</em> Ithaca, NY:
Shaker.</li>

<li>P&eacute;rez Carballo, A., 2018, &ldquo;Good Questions&rdquo;, in
J. Dunn &amp; K. Ahlstrom-Vij (eds.), <em>Epistemic
Consequentialism</em>, Oxford: Oxford University Press.</li>

<li>Pettigrew, R., 2010, &ldquo;Modelling uncertainty&rdquo;,
<em>Grazer Philosophische Studien</em>, 80.</li>

<li>&ndash;&ndash;&ndash;, 2013a, &ldquo;A New Epistemic Utility
Argument for the Principal Principle&rdquo;, <em>Episteme</em>, 10(1):
19&ndash;35.</li>

<li>&ndash;&ndash;&ndash;, 2013b, &ldquo;Epistemic Utility and Norms
for Credence&rdquo;, <em>Philosophy Compass</em>, 8(10):
897&ndash;908.</li>

<li>&ndash;&ndash;&ndash;, 2014a, &ldquo;Accuracy and Evidence&rdquo;,
<em>Dialectica</em>.</li>

<li>&ndash;&ndash;&ndash;, 2014b, &ldquo;Accuracy, Risk, and the
Principle of Indifference&rdquo;, <em>Philosophy and Phenomenological
Research</em>.</li>

<li>&ndash;&ndash;&ndash;, 2016a, <em>Accuracy and the Laws of
Credence</em>, Oxford: Oxford University Press.</li>

<li>&ndash;&ndash;&ndash;, 2016b, &ldquo;Jamesian epistemology
formalised: An explication of &lsquo;The Will to Believe&rsquo;&rdquo;
<em>Episteme</em> 13(3): 253&ndash;268.</li>

<li>&ndash;&ndash;&ndash;, 2017, &ldquo;On the Accuracy of Group
Credences&rdquo; in T. Szab&oacute; Gendler &amp; J. Hawthorne (eds.)
<em>Oxford Studies in Epistemology, volume 6</em> Oxford: Oxford
University Press.</li>

<li>&ndash;&ndash;&ndash;, 2018a, &ldquo;Making Things Right: the true
consequences of decision theory in epistemology&rdquo;, in
Ahlstrom-Vij &amp; Dunn 2018.</li>

<li>&ndash;&ndash;&ndash;, 2018b, &ldquo;The Population Ethics of
Belief: In Search of an Epistemic Theory X&rdquo;,
<em>No&ucirc;s</em>, 52(2): 336&ndash;372.</li>

<li>&ndash;&ndash;&ndash;, 2018c, &ldquo;Accuracy-First Epistemology
Without Additivity&rdquo;, <em>Philosophy of Science</em>, 89(1):
128&ndash;151.</li>

<li>&ndash;&ndash;&ndash;, 2021a, &ldquo;Logical ignorance and logical
learning&rdquo;, <em>Synthese</em>, 198(10): 9991&ndash;10020.</li>

<li>&ndash;&ndash;&ndash;, 2021b, &ldquo;On the pragmatic and
epistemic virtues of inference to the best explanation&rdquo;,
<em>Synthese</em> 199(5&ndash;6): 12407&ndash;12438.</li>

<li>&ndash;&ndash;&ndash;, 2022, <em>Epistemic Risk and the Demands of
Rationality</em>, Oxford: Oxford University Press.</li>

<li>&ndash;&ndash;&ndash;, 2023, &ldquo;Bayesian updating when what
you learn might be false&rdquo;, <em>Erkenntnis</em> 88(1):
309&ndash;324.</li>

<li>Predd, J., et al., 2009, &ldquo;Probabilistic Coherence and Proper
Scoring Rules&rdquo;, <em>IEEE Transactions on Information Theory</em>
55(10): 4786&ndash;4792.</li>

<li>Quiggin, J., 1982, &ldquo;A theory of anticipated utility&rdquo;,
<em>Journal of Economic Behavior and Organization</em> 3(4):
323&ndash;43.</li>

<li>Raidl, E. &amp; W. Spohn, 2020, &ldquo;An Accuracy Argument in
Favor of Ranking Theory&rdquo;, <em>Journal of Philosophical
Logic</em> 49(2): 283&ndash;313.</li>

<li>Ramsey, F. P., 1926 [1931], &ldquo;Truth and Probability&rdquo;,
in R. B. Braithwaite (ed.) <em>The Foundations of Mathematics and
other Logical Essays</em>, London: Routledge &amp; Kegan Paul
Ltd.</li>

<li>Rescorla, M., 2022, &ldquo;An Improved Dutch Book Theorem for
Conditionalization&rdquo;, <em>Erkenntnis</em>, 87(3):
1013&ndash;1041.</li>

<li>Rosenkrantz, R.D., 1981, <em>Foundations and Applications of
Inductive Probability</em>, Atascadero, CA: Ridgeview Press.</li>

<li>Rothschild, D., 2021, &lsquo;Lockean Beliefs, Dutch Books, and
Scoring Systems&rsquo;, <em>Erkenntnis</em>, 88(5):
1979&ndash;1995.</li>

<li>Schervish, M. J., 1989, &ldquo;A General Method for Comparing
Probability Assessors&ldquo; <em>The Annals of Statistics</em>, 17(4):
1856&ndash;1879.</li>

<li>Schoenfield, M., 2014, &ldquo;Permission to Believe: Why
Permissivism Is True and What It Tells Us About Irrelevant Influences
on Belief&rdquo;, <em>No&ucirc;s</em>, 48(2): 193&ndash;218.</li>

<li>&ndash;&ndash;&ndash;, 2016, &ldquo;Conditionalization does not
(in general) Maximize Expected Accuracy&rdquo;, <em>Mind</em>,
126(504): 1155&ndash;1187</li>

<li>&ndash;&ndash;&ndash;, 2017, &ldquo;The Accuracy and Rationality
of Imprecise Credences&rdquo;, <em>No&ucirc;s</em>, 51(4):
667&ndash;685.</li>

<li>&ndash;&ndash;&ndash;, 2019, &ldquo;Accuracy and Verisimilitude:
The Good, The Bad, and The Ugly&rdquo;, <em>The British Journal for
the Philosophy of Science</em>. doi:10.1093/bjps/axz032</li>

<li>Seidenfeld, T., 1985, &ldquo;Calibration, Coherence, and Scoring
Rules&rdquo;, <em>Philosophy of Science</em>, 52(2):
274&ndash;294.</li>

<li>Seidenfeld, T., M.J. Schervish, &amp; J.B. Kadane, 2012,
&ldquo;Forecasting with imprecise probabilities&rdquo;,
<em>International Journal of Approximate Reasoning</em>, 53:
1248&ndash;1261.</li>

<li>Shear, T. &amp; B. Fitelson, 2019, &ldquo;Two Approaches to Belief
Revision&rdquo; <em>Erkenntnis</em> 84(3): 487&ndash;518.</li>

<li>Spohn, W., 2012, <em>The Laws of Belief: Ranking Theory &amp; its
Philosophical Applications</em>, New York: Oxford University
Press.</li>

<li>Staffel, J. &amp; G. de Bona, Forthcoming, &ldquo;An Improved
Argument for Superconditionalization&rdquo;, <em>Erkenntnis</em>.</li>

<li>Sylvan, K., 2020, &ldquo;An Epistemic Non-Consequentialism&rdquo;,
<em>The Philosophical Review</em>, 129(1): 1&ndash;51.</li>

<li>Talbot, B., 2022, I &ldquo;Headaches for Epistemologists&rdquo;,
<em>Philosophy and Phenomenological Research</em>, 104(2):
408&ndash;433.</li>

<li>van Fraassen, B. C., 1999, &ldquo;Conditionalization, A New
Argument For&rdquo;, <em>Topoi</em>, 18(2): 93&ndash;96.</li>

<li>Vindrola, F. &amp; V. Crupi, forthcoming, &ldquo;Bayesians Too
Should Follow Wason: A Comprehensive Accuracy&ndash;Based Analysis of
the Selection Task&rdquo;, <em>The British Journal for the Philosophy
of Science</em>.</li>

<li>Wald, A., 1945, &ldquo;Statistical decision functions which
minimize the maximum risk&rdquo;, <em>The Annals of Mathematics</em>,
46(2): 265&ndash;280.</li>

<li>Walsh, S., ms., &ldquo;Probabilism in Infinite Dimensions&rdquo;.
Unpublished manuscript. </li>

<li>White, R., 2009, &ldquo;Evidential Symmetry and Mushy
Credence&rdquo;, <em>Oxford Studies in Epistemology</em>, 3:
161&ndash;186.</li>

<li>Williams, J. R. G., 2012a, &ldquo;Gradational accuracy and
nonclassical semantics&rdquo;, <em>Review of Symbolic Logic</em>,
5(4):513&ndash;537.</li>

<li>&ndash;&ndash;&ndash;, 2012b, &ldquo;Generalized Probabilism:
Dutch Books and accuracy domination&rdquo;, <em>Journal of
Philosophical Logic</em>, 41(5):811&ndash;840.</li>

<li>&ndash;&ndash;&ndash;, 2018, &ldquo;Rational Illogicality&rdquo;,
<em>Australasian Journal of Philosophy</em>, 96(1):
127&ndash;141.</li>

<li>Williams, J. R. G. &amp; R. Pettigrew, 2023, &lsquo;Consequences
of Calibration&rsquo;, <em>The British Journal for the Philosophy of
Science</em>.</li>

<li>Williamson, T., 2000, <em>Knowledge and its Limits</em>, Oxford:
Oxford University Press.</li>

<li>Wong, S.K.M., Yao, Y.Y., Bollmann, P., &amp; B&uuml;rger, H.C.,
1991, &ldquo;Axiomatization of qualitative belief structure&rdquo;,
<em>IEEE Transactions on System, Man, and Cybernetics</em>, 21:
726&ndash;734.</li>

<li>Zollman, K. J. S., 2010, &ldquo;The Epistemic Benefit of Transient
Diversity&rdquo;, <em>Erkenntnis</em> 72(1): 17&ndash;35.</li>
</ul>
</div> 

<div id="academic-tools">
<h2 id="Aca">Academic Tools</h2>

<blockquote>
<table class="vert-top">
<tr>
<td><img src="../../symbols/sepman-icon.jpg" alt="sep man icon" /></td>
<td><a href="https://plato.stanford.edu/cgi-bin/encyclopedia/archinfo.cgi?entry=epistemic-utility" target="other">How to cite this entry</a>.</td>
</tr>

<tr>
<td><img src="../../symbols/sepman-icon.jpg" alt="sep man icon" /></td>
<td><a href="https://leibniz.stanford.edu/friends/preview/epistemic-utility/" target="other">Preview the PDF version of this entry</a> at the
 <a href="https://leibniz.stanford.edu/friends/" target="other">Friends of the SEP Society</a>.</td>
</tr>

<tr>
<td><img src="../../symbols/inpho.png" alt="inpho icon" /></td>
<td><a href="https://www.inphoproject.org/entity?sep=epistemic-utility&amp;redirect=True" target="other">Look up topics and thinkers related to this entry</a>
 at the Internet Philosophy Ontology Project (InPhO).</td>
</tr>

<tr>
<td><img src="../../symbols/pp.gif" alt="phil papers icon" /></td>
<td><a href="https://philpapers.org/sep/epistemic-utility/" target="other">Enhanced bibliography for this entry</a>
at <a href="https://philpapers.org/" target="other">PhilPapers</a>, with links to its database.</td>
</tr>

</table>
</blockquote>
</div>

<div id="other-internet-resources">

<h2><a id="Oth">Other Internet Resources</a></h2>

<ul>

<li>Pettigrew, Richard,
 <a href="https://richardpettigrew.com/books/accuracy/" target="other">The webpage for Pettigrew&rsquo;s <em>Accuracy and the Laws of Credence</em></a>.
 This includes video tutorials that work through some of the central
results in accuracy-first epistemology.</li>

<li>Weisberg, Jonathan,
 <a href="https://jonathanweisberg.org/tags/accuracy-for-dummies/" target="other">A series of blogposts</a>
 that walk slowly through the technical side of accuracy-first
epistemology.</li>
</ul>
</div>

<div id="related-entries">

<h2><a id="Rel">Related Entries</a></h2>

<p>

 <a href="../formal-belief/">belief, formal representations of</a> |
 <a href="../epistemology/">epistemology</a> |
 <a href="../epistemology-bayesian/">epistemology: Bayesian</a> |
 <a href="../probability-interpret/">probability, interpretations of</a>

</p>
</div>
<script type="text/javascript" src="local.js"></script>
<script type="text/javascript" src="../../MathJax/MathJax.js?config=TeX-MML-AM_CHTML"></script>


</div><!-- #aueditable --><!--DO NOT MODIFY THIS LINE AND BELOW-->

<!-- END ARTICLE HTML -->

</div> <!-- End article-content -->

  <div id="article-copyright">
    <p>
 <a href="../../info.html#c">Copyright &copy; 2023</a> by

<br />
<a href="http://eis.bris.ac.uk/~rp3959/" target="other">Richard Pettigrew</a>
&lt;<a href="m&#97;ilto:richard&#37;2epettigrew&#37;40bris&#37;2eac&#37;2euk"><em>richard<abbr title=" dot ">&#46;</abbr>pettigrew<abbr title=" at ">&#64;</abbr>bris<abbr title=" dot ">&#46;</abbr>ac<abbr title=" dot ">&#46;</abbr>uk</em></a>&gt;
    </p>
  </div>

</div> <!-- End article -->

<!-- NOTE: article banner is outside of the id="article" div. -->
<div id="article-banner" class="scroll-block">
  <div id="article-banner-content">
    <a href="../../fundraising/">
    Open access to the SEP is made possible by a world-wide funding initiative.<br />
    The Encyclopedia Now Needs Your Support<br />
    Please Read How You Can Help Keep the Encyclopedia Free</a>
  </div>
</div> <!-- End article-banner -->

    </div> <!-- End content -->

    <div id="footer">

      <div id="footer-menu">
        <div class="menu-block">
          <h4><i class="icon-book"></i> Browse</h4>
          <ul role="menu">
            <li role="menuitem"><a href="../../contents.html">Table of Contents</a></li>
            <li role="menuitem"><a href="../../new.html">What's New</a></li>
            <li role="menuitem"><a href="https://plato.stanford.edu/cgi-bin/encyclopedia/random">Random Entry</a></li>
            <li role="menuitem"><a href="../../published.html">Chronological</a></li>
            <li role="menuitem"><a href="../../archives/">Archives</a></li>
          </ul>
        </div>
        <div class="menu-block">
          <h4><i class="icon-info-sign"></i> About</h4>
          <ul role="menu">
            <li role="menuitem"><a href="../../info.html">Editorial Information</a></li>
            <li role="menuitem"><a href="../../about.html">About the SEP</a></li>
            <li role="menuitem"><a href="../../board.html">Editorial Board</a></li>
            <li role="menuitem"><a href="../../cite.html">How to Cite the SEP</a></li>
            <li role="menuitem"><a href="../../special-characters.html">Special Characters</a></li>
            <li role="menuitem"><a href="../../tools/">Advanced Tools</a></li>
            <li role="menuitem"><a href="../../accessibility.html">Accessibility</a></li>
            <li role="menuitem"><a href="../../contact.html">Contact</a></li>
          </ul>
        </div>
        <div class="menu-block">
          <h4><i class="icon-leaf"></i> Support SEP</h4>
          <ul role="menu">
            <li role="menuitem"><a href="../../support/">Support the SEP</a></li>
            <li role="menuitem"><a href="../../support/friends.html">PDFs for SEP Friends</a></li>
            <li role="menuitem"><a href="../../support/donate.html">Make a Donation</a></li>
            <li role="menuitem"><a href="../../support/sepia.html">SEPIA for Libraries</a></li>
          </ul>
        </div>
      </div> <!-- End footer menu -->

      <div id="mirrors">
        <div id="mirror-info">
          <h4><i class="icon-globe"></i> Mirror Sites</h4>
          <p>View this site from another server:</p>
        </div>
        <div class="btn-group open">
          <a class="btn dropdown-toggle" data-toggle="dropdown" href="https://plato.stanford.edu/">
            <span class="flag flag-usa"></span> USA (Main Site) <span class="caret"></span>
            <span class="mirror-source">Philosophy, Stanford University</span>
          </a>
          <ul class="dropdown-menu">
            <li><a href="../../mirrors.html">Info about mirror sites</a></li>
          </ul>
        </div>
      </div> <!-- End mirrors -->
      
      <div id="site-credits">
        <p>The Stanford Encyclopedia of Philosophy is <a href="../../info.html#c">copyright &copy; 2023</a> by <a href="https://mally.stanford.edu/">The Metaphysics Research Lab</a>, Department of Philosophy, Stanford University</p>
        <p>Library of Congress Catalog Data: ISSN 1095-5054</p>
      </div> <!-- End site credits -->

    </div> <!-- End footer -->

  </div> <!-- End container -->

   <!-- NOTE: Script required for drop-down button to work (mirrors). -->
  <script>
    $('.dropdown-toggle').dropdown();
  </script>

</body>
</html>
